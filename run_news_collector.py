#!/usr/bin/env python3
"""
æ–°èæ”¶é›†å™¨ - æ··åˆç­–ç•¥å¯¦ä½œ
å®šæœŸçˆ¬å–æ–°èä¸¦å­˜å„²åˆ°è³‡æ–™åº«ï¼Œä¾›å¾ŒçºŒæ¨é€ä½¿ç”¨
æ¡ç”¨ï¼šæ ¸å¿ƒè²¡ç¶“æ–°è + ç”¨æˆ¶é—œéµå­—ç›¸é—œæ–°è
"""

import sys
import os
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Set

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from scraper.scraper import NewsScraperManager
from core.config import settings
from core.database import db_manager
from core.utils import get_current_taiwan_time, format_taiwan_datetime, parse_article_publish_time, generate_summary_optimized

class NewsCollector:
    """æ··åˆç­–ç•¥æ–°èæ”¶é›†å™¨"""
    
    def __init__(self):
        self.scraper = NewsScraperManager()
        
    def get_all_user_keywords(self) -> Set[str]:
        """ç²å–æ‰€æœ‰æ´»èºç”¨æˆ¶çš„é—œéµå­—"""
        try:
            subscriptions = db_manager.get_active_subscriptions()
            all_keywords = set()
            
            for sub in subscriptions:
                keywords = sub.get('keywords', [])
                if keywords:
                    all_keywords.update([kw.lower().strip() for kw in keywords])
            
            print(f"ğŸ“‹ æ”¶é›†åˆ°ç”¨æˆ¶é—œéµå­—: {len(all_keywords)} å€‹")
            return all_keywords
            
        except Exception as e:
            print(f"âŒ ç²å–ç”¨æˆ¶é—œéµå­—å¤±æ•—: {e}")
            return set()
    
    def collect_core_articles(self, limit: int = 20) -> List[dict]:
        """æ”¶é›†æ ¸å¿ƒè²¡ç¶“æ–°èï¼ˆå‰Nç¯‡ï¼Œä¿è­‰åŸºç¤å…§å®¹ï¼‰"""
        print(f"\nğŸ“° é–‹å§‹æ”¶é›†æ ¸å¿ƒè²¡ç¶“æ–°è (å‰ {limit} ç¯‡)...")
        
        news_list = self.scraper.scrape_yahoo_finance_list()
        if not news_list:
            print("âŒ æœªèƒ½çˆ¬å–åˆ°æ–°èåˆ—è¡¨")
            return []
        
        core_articles = []
        processed_count = 0
        
        for news_item in news_list[:limit]:  # åªè™•ç†å‰Nç¯‡
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
            if db_manager.is_article_processed(news_item['link']):
                continue
                
            processed_count += 1
            print(f"ğŸ”„ è™•ç†æ ¸å¿ƒæ–°è ({processed_count}/{limit}): {news_item['title'][:50]}...")
            
            article_data = self._process_single_article(news_item, article_type="core")
            if article_data:
                core_articles.append(article_data)
                print(f"âœ… æ ¸å¿ƒæ–°èè™•ç†æˆåŠŸ")
            else:
                print(f"âŒ æ ¸å¿ƒæ–°èè™•ç†å¤±æ•—")
                
            # å¦‚æœå·²æ”¶é›†è¶³å¤ çš„æ ¸å¿ƒæ–‡ç« ï¼Œæå‰çµæŸ
            if len(core_articles) >= limit // 2:  # è‡³å°‘æˆåŠŸä¸€åŠ
                break
        
        print(f"ğŸ“Š æ ¸å¿ƒæ–°èæ”¶é›†å®Œæˆ: {len(core_articles)} ç¯‡æˆåŠŸ")
        return core_articles
    
    def collect_keyword_articles(self, keywords: Set[str], limit: int = 30) -> List[dict]:
        """æ”¶é›†é—œéµå­—ç›¸é—œæ–‡ç« """
        if not keywords:
            print("ğŸ“‹ æ²’æœ‰ç”¨æˆ¶é—œéµå­—ï¼Œè·³éé—œéµå­—æ–‡ç« æ”¶é›†")
            return []
            
        print(f"\nğŸ” é–‹å§‹æ”¶é›†é—œéµå­—ç›¸é—œæ–‡ç«  (é—œéµå­—: {len(keywords)} å€‹ï¼Œæœ€å¤š {limit} ç¯‡)...")
        print(f"ğŸ·ï¸ é—œéµå­—: {', '.join(list(keywords)[:5])}{'...' if len(keywords) > 5 else ''}")
        
        news_list = self.scraper.scrape_yahoo_finance_list()
        if not news_list:
            return []
        
        keyword_articles = []
        processed_count = 0
        
        for news_item in news_list:
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
            if db_manager.is_article_processed(news_item['link']):
                continue
            
            # æª¢æŸ¥é—œéµå­—åŒ¹é…
            title_lower = news_item['title'].lower()
            matched_keywords = [kw for kw in keywords if kw in title_lower]
            
            if not matched_keywords:
                continue
                
            processed_count += 1
            print(f"ğŸ¯ è™•ç†é—œéµå­—æ–‡ç«  ({processed_count}): {news_item['title'][:50]}...")
            print(f"   åŒ¹é…é—œéµå­—: {', '.join(matched_keywords)}")
            
            article_data = self._process_single_article(news_item, article_type="keyword", matched_keywords=matched_keywords)
            if article_data:
                keyword_articles.append(article_data)
                print(f"âœ… é—œéµå­—æ–‡ç« è™•ç†æˆåŠŸ")
            else:
                print(f"âŒ é—œéµå­—æ–‡ç« è™•ç†å¤±æ•—")
                
            # å¦‚æœå·²æ”¶é›†è¶³å¤ çš„é—œéµå­—æ–‡ç« ï¼ŒçµæŸ
            if len(keyword_articles) >= limit:
                break
        
        print(f"ğŸ“Š é—œéµå­—æ–‡ç« æ”¶é›†å®Œæˆ: {len(keyword_articles)} ç¯‡æˆåŠŸ")
        return keyword_articles
    
    def _process_single_article(self, news_item: dict, article_type: str = "core", matched_keywords: List[str] = None) -> dict:
        """è™•ç†å–®ç¯‡æ–‡ç« """
        try:
            # çˆ¬å–æ–‡ç« å…§å®¹
            print(f"  ğŸ“¥ é–‹å§‹çˆ¬å–æ–‡ç« å…§å®¹...")
            content = self.scraper.scrape_article_content(news_item['link'])
            if not content:
                print(f"  âŒ æ–‡ç« å…§å®¹çˆ¬å–å¤±æ•—")
                return None
            
            print(f"  âœ… æ–‡ç« å…§å®¹çˆ¬å–æˆåŠŸ ({len(content)} å­—)")
            
            # ç”Ÿæˆæ‘˜è¦
            print(f"  ğŸ¤– é–‹å§‹ç”ŸæˆAIæ‘˜è¦...")
            summary = generate_summary_optimized(content)
            if "[æ‘˜è¦ç”Ÿæˆå¤±æ•—" in summary:
                print(f"  âŒ AIæ‘˜è¦ç”Ÿæˆå¤±æ•—")
                return None
            
            print(f"  âœ… AIæ‘˜è¦ç”ŸæˆæˆåŠŸ")
            
            # è§£æç™¼å¸ƒæ™‚é–“
            published_at = parse_article_publish_time()
            
            # æ§‹å»ºæ–‡ç« æ•¸æ“š
            article_data = {
                'original_url': news_item['link'],
                'source': 'yahoo_finance',
                'title': news_item['title'],
                'summary': summary,
                'published_at': published_at.isoformat(),
                # æ·»åŠ å…ƒæ•¸æ“šç”¨æ–¼å¾ŒçºŒæ¨é€é‚è¼¯
                'collection_type': article_type,
                'matched_keywords': matched_keywords or []
            }
            
            return article_data
            
        except Exception as e:
            print(f"âŒ è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def save_articles_to_database(self, articles: List[dict]) -> int:
        """å°‡æ–‡ç« æ‰¹é‡ä¿å­˜åˆ°è³‡æ–™åº«"""
        if not articles:
            return 0
            
        print(f"\nğŸ’¾ é–‹å§‹ä¿å­˜ {len(articles)} ç¯‡æ–‡ç« åˆ°è³‡æ–™åº«...")
        success_count = 0
        
        for article in articles:
            # ç§»é™¤å…ƒæ•¸æ“šï¼ˆè³‡æ–™åº«ä¸éœ€è¦ï¼‰
            db_article = {k: v for k, v in article.items() 
                         if k not in ['collection_type', 'matched_keywords']}
            
            article_id = db_manager.save_new_article(db_article)
            if article_id:
                success_count += 1
        
        print(f"âœ… æˆåŠŸä¿å­˜ {success_count} ç¯‡æ–‡ç« åˆ°è³‡æ–™åº«")
        return success_count
    
    def run_collection(self) -> bool:
        """åŸ·è¡Œæ··åˆç­–ç•¥æ–°èæ”¶é›†"""
        print("=" * 60)
        print("ğŸ“° FinNews-Bot æ–°èæ”¶é›†å™¨é–‹å§‹åŸ·è¡Œ")
        taiwan_time = get_current_taiwan_time()
        print(f"ğŸ• åŸ·è¡Œæ™‚é–“: {format_taiwan_datetime(taiwan_time)}")
        print("=" * 60)
        
        try:
            # é©—è­‰ç’°å¢ƒè®Šæ•¸
            settings.validate()
            print("âœ… ç’°å¢ƒè®Šæ•¸é©—è­‰æˆåŠŸ")
            
            # 1. æ”¶é›†æ ¸å¿ƒè²¡ç¶“æ–°è
            core_articles = self.collect_core_articles(limit=20)
            
            # 2. æ”¶é›†ç”¨æˆ¶é—œéµå­—ç›¸é—œæ–‡ç« 
            user_keywords = self.get_all_user_keywords()
            keyword_articles = self.collect_keyword_articles(user_keywords, limit=30)
            
            # 3. åˆä½µå»é‡
            all_articles = []
            seen_urls = set()
            
            # å„ªå…ˆæ·»åŠ é—œéµå­—æ–‡ç« ï¼ˆç”¨æˆ¶æ›´æ„Ÿèˆˆè¶£ï¼‰
            for article in keyword_articles:
                if article['original_url'] not in seen_urls:
                    all_articles.append(article)
                    seen_urls.add(article['original_url'])
            
            # æ·»åŠ æ ¸å¿ƒæ–‡ç« 
            for article in core_articles:
                if article['original_url'] not in seen_urls:
                    all_articles.append(article)
                    seen_urls.add(article['original_url'])
            
            print(f"\nğŸ“Š æ”¶é›†ç¸½çµ:")
            print(f"  - æ ¸å¿ƒæ–‡ç« : {len(core_articles)} ç¯‡")
            print(f"  - é—œéµå­—æ–‡ç« : {len(keyword_articles)} ç¯‡")
            print(f"  - å»é‡å¾Œç¸½è¨ˆ: {len(all_articles)} ç¯‡")
            
            # 4. ä¿å­˜åˆ°è³‡æ–™åº«
            if all_articles:
                saved_count = self.save_articles_to_database(all_articles)
                
                if saved_count > 0:
                    print(f"\nğŸ‰ æ–°èæ”¶é›†ä»»å‹™å®Œæˆï¼å…±æ”¶é›† {saved_count} ç¯‡æ–°æ–‡ç« ")
                    return True
                else:
                    print(f"\nâš ï¸ æ–°èæ”¶é›†å®Œæˆï¼Œä½†æœªèƒ½ä¿å­˜ä»»ä½•æ–‡ç« ")
                    return False
            else:
                print(f"\nğŸ“­ æœ¬æ¬¡æ”¶é›†æœªç²å¾—æ–°æ–‡ç« ï¼ˆå¯èƒ½éƒ½å·²å­˜åœ¨æ–¼è³‡æ–™åº«ï¼‰")
                return False
                
        except Exception as e:
            print(f"\nâŒ æ–°èæ”¶é›†ä»»å‹™å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False
            
        finally:
            print("\n" + "=" * 60)
            print("ğŸ FinNews-Bot æ–°èæ”¶é›†å™¨çµæŸ")
            taiwan_time = get_current_taiwan_time()
            print(f"â° çµæŸæ™‚é–“: {format_taiwan_datetime(taiwan_time)}")
            print("=" * 60)

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    collector = NewsCollector()
    success = collector.run_collection()
    return 0 if success else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)