import os
import random
import time
import logging
from pathlib import Path
from typing import Union, List, Dict, Any, Tuple
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

from scraper.scraper_v2 import ScraperV2
from core.config import settings
from core.database import db_manager
from core.utils import parse_article_publish_time
from core.translation_service import translate_title_to_chinese
from core.logger_config import setup_logging

# Initialize logger
logger = logging.getLogger(__name__)

class NewsScraperManager:
    """News scraper manager for FinNews-Bot"""
    
    def __init__(self, use_v2_scraper: bool = False):
        self.debug_folder = self._create_debug_folder()
        self.use_v2_scraper = use_v2_scraper
        if self.use_v2_scraper:
            self.scraper_v2 = ScraperV2()
    
    def _create_debug_folder(self) -> Path:
        path = Path("debug_pages")
        path.mkdir(exist_ok=True)
        return path

    def collect_news_from_topics(self, targets: List[Dict[str, str]], max_articles_to_process: int = None) -> Tuple[bool, Dict[str, int]]:
        """æ–°çš„ä¸»åŸ·è¡Œå‡½å¼ - æ ¹æ“šç›®æ¨™ä¸»é¡Œåˆ—è¡¨æ”¶é›†æ–°è"""
        stats = {
            'total_processed': 0,
            'newly_added': 0,
            'duplicates': 0,
            'failed': 0
        }
        articles_to_save = []
        processed_urls_in_run = set() # æ–°å¢: ç”¨æ–¼è¿½è¹¤æœ¬æ¬¡é‹è¡Œä¸­å·²è™•ç†çš„URL

        for target in targets:
            topic_code = target['topic_code']
            url = target['url']
            logger.info(f"--- é–‹å§‹è™•ç†ä¸»é¡Œ: {topic_code} ---")
            
            news_list = self.scrape_yahoo_finance_list(url)
            if not news_list:
                logger.warning(f"æœªèƒ½å¾ {url} çˆ¬å–åˆ°ä»»ä½•æ–°èåˆ—è¡¨ã€‚")
                continue

            processed_for_topic = 0
            for news_item in news_list:
                # æ–°å¢æª¢æŸ¥: å¦‚æœæ­¤URLåœ¨æœ¬è¼ªé‹è¡Œä¸­å·²è™•ç†ï¼Œå‰‡è·³é
                if news_item['link'] in processed_urls_in_run:
                    logger.info(f"æ–‡ç« åœ¨æœ¬è¼ªå·²è™•ç† (SKIP): {news_item['title'][:50]}...")
                    continue

                if max_articles_to_process is not None and processed_for_topic >= max_articles_to_process:
                    logger.info(f"å·²é”åˆ°ä¸»é¡Œ {topic_code} çš„è™•ç†ä¸Šé™ ({max_articles_to_process} ç¯‡æ–‡ç« )ï¼Œè·³éå‰©é¤˜æ–‡ç« ã€‚")
                    break

                stats['total_processed'] += 1
                
                if db_manager.is_article_processed(news_item['link']):
                    logger.info(f"æ–‡ç« å·²åœ¨è³‡æ–™åº«ä¸­ (SKIP): {news_item['title'][:50]}...")
                    stats['duplicates'] += 1
                    processed_urls_in_run.add(news_item['link']) # ä¹Ÿå°‡å…¶åŠ å…¥ï¼Œé¿å…é‡è¤‡è™•ç†
                    continue

                logger.info(f"æ­£åœ¨è™•ç†æ–°æ–‡ç« : {news_item['title'][:50]}...")
                article_data = self._process_single_article(news_item, topic_code)
                
                if article_data:
                    articles_to_save.append(article_data)
                    processed_urls_in_run.add(news_item['link']) # æ–°å¢: è¨˜éŒ„å·²è™•ç†çš„URL
                    processed_for_topic += 1
                else:
                    stats['failed'] += 1

        if articles_to_save:
            logger.info(f"æº–å‚™å°‡ {len(articles_to_save)} ç¯‡æ–‡ç« é€²è¡Œæ‰¹æ¬¡å„²å­˜...")
            success, count = db_manager.save_new_articles_batch(articles_to_save)
            if success:
                stats['newly_added'] = count
                if count < len(articles_to_save):
                    stats['failed'] += (len(articles_to_save) - count)
            else:
                stats['failed'] += len(articles_to_save)

        return True, stats

    def scrape_yahoo_finance_list(self, url: str) -> List[Dict[str, str]]:
        """ä½¿ç”¨ requests çˆ¬å– Yahoo Finance çš„æ–°èåˆ—è¡¨é """
        logger.info(f"æ­£åœ¨å¾ {url} çˆ¬å–æ–°èåˆ—è¡¨...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select("li.stream-item")
            result = []
            seen_links = set()
            
            for item in items:
                a_tag = item.select_one('a[href*="/news/"]')
                title_tag = item.select_one('h3')
                if a_tag and title_tag:
                    link = requests.compat.urljoin(url, a_tag['href'])
                    if link not in seen_links:
                        seen_links.add(link)
                        result.append({
                            'title': title_tag.get_text(strip=True), 
                            'link': link
                        })
            
            logger.info(f"æˆåŠŸçˆ¬å–åˆ° {len(result)} å‰‡æ–°èæ¨™é¡Œã€‚")
            return result
        except Exception as e:
            logger.exception(f"çˆ¬å–æ–°èåˆ—è¡¨å¤±æ•—: {url}")
            return []
    
    def _process_single_article(self, news_item: Dict[str, str], topic_code: str) -> Union[Dict[str, Any], None]:
        """è™•ç†å–®ç¯‡æ–‡ç«  - çˆ¬å–å…§å®¹ã€ç”Ÿæˆæ‘˜è¦/æ¨™ç±¤ï¼Œä¸¦é™„ä¸Šä¾†æºä¸»é¡Œ"""
        try:
            content = self.scrape_article_content(news_item['link'])
            if not content:
                return None

            summary, tags = self.generate_summary_and_tags(news_item['title'], content)
            if "[æ‘˜è¦ç”Ÿæˆå¤±æ•—" in summary:
                return None

            # ğŸŒ æ–°å¢ç¿»è­¯åŠŸèƒ½ï¼šç¿»è­¯æ¨™é¡Œç‚ºä¸­æ–‡
            translated_title = None
            try:
                logger.info(f"ğŸŒ é–‹å§‹ç¿»è­¯æ¨™é¡Œ: {news_item['title'][:50]}...")
                translated_title = translate_title_to_chinese(news_item['title'])
                
                if translated_title:
                    logger.info(f"âœ… ç¿»è­¯æˆåŠŸ: {translated_title[:50]}...")
                else:
                    logger.info("â„¹ï¸ æ¨™é¡Œç„¡éœ€ç¿»è­¯æˆ–ç¿»è­¯å¤±æ•—ï¼Œå°‡å„²å­˜åŸæ¨™é¡Œ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ç¿»è­¯éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼Œä½†ä¸å½±éŸ¿æ–‡ç« è™•ç†: {e}")
                translated_title = None

            published_at = parse_article_publish_time()
            
            logger.debug(f"Inside _process_single_article - Type of news_item['link']: {type(news_item['link'])}")
            logger.debug(f"Inside _process_single_article - Type of tags: {type(tags)}")

            article_data = {
                'original_url': news_item['link'], 
                'source': 'yahoo_finance', 
                'title': news_item['title'], 
                'summary': summary,
                'translated_title': translated_title,  # ğŸŒ æ–°å¢ç¿»è­¯æ¨™é¡Œæ¬„ä½
                'tags': tags,
                'topics': [topic_code],
                'published_at': published_at.isoformat()
            }
            
            return article_data
            
        except Exception as e:
            logger.exception(f"è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {news_item.get('link', 'N/A')}")
            return None

    def scrape_article_content(self, url: str) -> Union[str, None]:
        if self.use_v2_scraper:
            logger.info(f"ä½¿ç”¨ ScraperV2 æŠ“å–: {url[:70]}...")
            result = self.scraper_v2._scrape_single_article(url)
            if result['success']:
                return result['content']
            else:
                logger.error(f"ScraperV2 æŠ“å–å¤±æ•—: {result['error']} for url: {url}")
                return None
        else:
            logger.info(f"ä½¿ç”¨ Selenium æ­£åœ¨å•Ÿå‹•ç€è¦½å™¨æŠ“å–: {url[:70]}...")
            
            chrome_options = Options()
            chrome_options.add_argument("--headless=new")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1024,768")
            chrome_options.add_argument("user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36")

            driver = None
            try:
                driver = webdriver.Chrome(options=chrome_options)
                driver.set_page_load_timeout(settings.SCRAPER_TIMEOUT)
                driver.get(url)

                try:
                    consent_button_locator = (By.CSS_SELECTOR, "button.consent-button[value='agree'], button[name='agree']")
                    consent_button = WebDriverWait(driver, 10).until(
                        EC.element_to_be_clickable(consent_button_locator)
                    )
                    driver.execute_script("arguments[0].click();", consent_button)
                    time.sleep(random.uniform(1, 2))
                except TimeoutException:
                    logger.debug("No consent button found.")
                    pass

                content_container_locator = (By.CSS_SELECTOR, '[data-testid="article-content-wrapper"], div.caas-body')
                WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located(content_container_locator)
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                body = soup.select_one('[data-testid="article-content-wrapper"], div.caas-body')

                if body:
                    content_text = "\n".join(p.get_text(strip=True) for p in body.find_all("p") if p.get_text(strip=True))
                    if content_text:
                        logger.info(f"æˆåŠŸæ“·å–æ–‡ç« å…§æ–‡ï¼Œç´„ {len(content_text)} å­—ã€‚")
                        return content_text
                
                logger.warning(f"æœªæ‰¾åˆ°ä¸»è¦å…§æ–‡å®¹å™¨ for url: {url}")
                return None

            except Exception as e:
                logger.exception(f"ä½¿ç”¨ Selenium æ“·å–å…§æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {url}")
                return None
            finally:
                if driver:
                    driver.quit()
    
    def generate_summary_and_tags(self, title: str, content: str) -> tuple:
        """åŒæ™‚ç”Ÿæˆæ‘˜è¦å’ŒAIæ¨™ç±¤"""
        api_key = os.environ.get('OPENAI_API_KEY')
        if not api_key:
            logger.error("OPENAI_API_KEY not found, cannot generate summary and tags.")
            return f"Summary generation failed. Title: {title}", []
        
        # ä½¿ç”¨çµ±ä¸€æ¨™ç±¤ç®¡ç†ç³»çµ±
        try:
            from scripts.dynamic_tags import get_tags_for_scraper
            core_tags = get_tags_for_scraper()
        except Exception as e:
            logger.warning(f"Failed to load dynamic tags, using fallback: {e}")
            # é™ç´šåˆ°ç¡¬ç·¨ç¢¼æ¨™ç±¤ä½œç‚ºå‚™ç”¨
            core_tags = [
                "APPLE", "TSMC", "TESLA", "AI_TECH", "CRYPTO",
                "STOCK_MARKET", "ECONOMIES", "LATEST", "EARNINGS", 
                "TECH", "ELECTRIC_VEHICLES", "FEDERAL_RESERVE",
                "HOUSING", "ENERGY", "HEALTHCARE", "FINANCE",
                "TARIFFS", "TRADE", "COMMODITIES", "BONDS"
            ]
        
        prompt = f'''
è«‹ç‚ºä»¥ä¸‹è²¡ç¶“æ–°èåŒæ™‚å®Œæˆå…©å€‹ä»»å‹™ï¼š
æ–°èæ¨™é¡Œï¼š{title}
æ–°èå…§å®¹ï¼š{content[:1500]}
ä»»å‹™1 - ç”Ÿæˆæ‘˜è¦ï¼š
- ä½¿ç”¨ç¹é«”ä¸­æ–‡, 80-120å­—ä¹‹é–“, å®¢è§€ä¸­ç«‹, çªå‡ºé—œéµè³‡è¨Š
ä»»å‹™2 - åˆ†é…æ¨™ç±¤ï¼š
å¾ä»¥ä¸‹æ¨™ç±¤åº«é¸æ“‡æœ€ç›¸é—œçš„ï¼ˆæœ€å¤š3å€‹ï¼‰ï¼š{core_tags}
è«‹è¿”å›JSONæ ¼å¼ï¼š
{{
  "summary": "æ‘˜è¦å…§å®¹...",
  "tags": ["TAG1", "TAG2"],
  "confidence": 0.9
}}
'''
        headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}
        data = {'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': prompt}], 'max_tokens': 300, 'temperature': 0.2}
        
        import json
        try:
            response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, json=data, timeout=30)
            if response.status_code == 200:
                result = response.json()['choices'][0]['message']['content']
                try:
                    parsed = json.loads(result)
                    summary = parsed.get('summary', f"æ‘˜è¦è§£æå¤±æ•—ã€‚åŸæ¨™é¡Œï¼š{title}")
                    tags = parsed.get('tags', [])
                    if not isinstance(tags, list):
                        tags = []
                    return summary, tags
                except json.JSONDecodeError:
                    logger.error(f"JSON è§£æå¤±æ•—ã€‚åŸå§‹å›æ‡‰: {result}")
                    return f"JSON è§£æå¤±æ•—ã€‚åŸæ¨™é¡Œï¼š{title}", []
            else:
                logger.error(f"OpenAI API éŒ¯èª¤ã€‚ç‹€æ…‹ç¢¼: {response.status_code}, å›æ‡‰: {response.text}")
                return f"API error. Title: {title}", []
        except requests.exceptions.RequestException as e:
            logger.exception("è«‹æ±‚ OpenAI API å¤±æ•—")
            return f"è«‹æ±‚å¤±æ•—ã€‚åŸæ¨™é¡Œï¼š{title}", []
        except Exception as e:
            logger.exception("è™•ç† OpenAI å›æ‡‰æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤")
            return f"è™•ç†å¤±æ•—ã€‚åŸæ¨™é¡Œï¼š{title}", []

# Setup logging
setup_logging()

# Create a global scraper manager instance
scraper_manager = NewsScraperManager()