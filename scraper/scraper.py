import os
import random
import time
from pathlib import Path
from typing import Union, List, Dict, Any
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

from core.config import settings
from core.database import db_manager
from core.utils import generate_summary_optimized, send_batch_to_discord, create_push_summary_message, parse_article_publish_time

class NewsScraperManager:
    """News scraper manager for FinNews-Bot"""
    
    def __init__(self):
        self.debug_folder = self._create_debug_folder()
    
    def _create_debug_folder(self) -> Path:
        """å‰µå»ºä¸€å€‹ç”¨æ–¼å­˜æ”¾é™¤éŒ¯æˆªåœ–çš„è³‡æ–™å¤¾"""
        path = Path("debug_pages")
        path.mkdir(exist_ok=True)
        return path
    
    def scrape_yahoo_finance_list(self, url: str = None) -> List[Dict[str, str]]:
        """ä½¿ç”¨ requests çˆ¬å– Yahoo Finance çš„æ–°èåˆ—è¡¨é """
        if url is None:
            url = settings.YAHOO_FINANCE_URL
        
        print(f"News: æ­£åœ¨å¾ {url} çˆ¬å–æ–°èåˆ—è¡¨...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select("li.stream-item")
            result = []
            seen_links = set()
            
            for item in items:
                a_tag = item.select_one('a[href*="/news/"]')
                title_tag = item.select_one('h3')
                if a_tag and title_tag:
                    link = requests.compat.urljoin(url, a_tag['href'])
                    if link not in seen_links:
                        seen_links.add(link)
                        result.append({
                            'title': title_tag.get_text(strip=True), 
                            'link': link
                        })
            
            print(f"OK: æˆåŠŸçˆ¬å–åˆ° {len(result)} å‰‡æ–°èæ¨™é¡Œã€‚")
            return result
        except Exception as e:
            print(f"Error: çˆ¬å–æ–°èåˆ—è¡¨å¤±æ•—: {e}")
            return []
    
    def scrape_article_content(self, url: str) -> Union[str, None]:
        """
        ä½¿ç”¨ Selenium çˆ¬å–å–®ç¯‡æ–°èçš„å…§æ–‡
        - å¼·åŒ–å½è£ä»¥æ‡‰å°åçˆ¬èŸ²æ©Ÿåˆ¶
        - ä½¿ç”¨è¤‡åˆé¸æ“‡å™¨ä»¥æ‡‰å°å¤šç¨®é é¢ç‰ˆé¢
        """
        print(f"Selenium: [Selenium] æ­£åœ¨å•Ÿå‹•ç€è¦½å™¨æŠ“å–å®Œæ•´ URL: {url}")
        
        # å°å…¥å¿…è¦æ¨¡çµ„
        import os
        import shutil
        import stat
        import platform
        
        chrome_options = Options()
        
        # è¨˜æ†¶é«”å„ªåŒ–çš„Chromeè¨­å®š
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        
        # è¨˜æ†¶é«”é™åˆ¶è¨­å®š
        chrome_options.add_argument("--memory-pressure-off")
        chrome_options.add_argument("--max_old_space_size=1024")  # é™åˆ¶JSå †è¨˜æ†¶é«”ç‚º1GB
        chrome_options.add_argument("--aggressive-cache-discard")
        chrome_options.add_argument("--disable-background-timer-throttling")
        chrome_options.add_argument("--disable-renderer-backgrounding")
        chrome_options.add_argument("--disable-backgrounding-occluded-windows")
        chrome_options.add_argument("--disable-background-networking")
        
        # åŠŸèƒ½ç¦ç”¨ï¼ˆæ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨ï¼‰
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")  # ç¦ç”¨åœ–ç‰‡è¼‰å…¥
        chrome_options.add_argument("--disable-javascript")  # ç¦ç”¨JavaScriptï¼ˆæ–°èå…§å®¹é€šå¸¸åœ¨HTMLä¸­ï¼‰
        chrome_options.add_argument("--disable-css")  # ç¦ç”¨CSS
        chrome_options.add_argument("--disable-features=TranslateUI,VizDisplayCompositor")
        chrome_options.add_argument("--disable-ipc-flooding-protection")
        
        # çª—å£å’Œæ¸²æŸ“è¨­å®š
        chrome_options.add_argument("--window-size=1024,768")  # æ¸›å°çª—å£å°ºå¯¸
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--ignore-certificate-errors")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        
        # GitHub Actionsç‰¹æ®Šè¨­å®š
        if os.environ.get('GITHUB_ACTIONS'):
            chrome_options.add_argument("--single-process")  # å¼·åˆ¶å–®é€²ç¨‹æ¨¡å¼
            chrome_options.add_argument("--disable-software-rasterizer")
            chrome_options.add_argument("--disable-background-networking")
            print("ğŸ”§ GitHub Actionsç’°å¢ƒï¼šå•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼")
            
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # è¨­å®šUser-Agent
        chrome_options.add_argument("user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36")

        driver = None
        try:
            # ä½¿ç”¨Selenium 4.6+å…§å»ºçš„driverç®¡ç†ï¼ˆä¸éœ€è¦webdriver-managerï¼‰
            print("Using Selenium built-in ChromeDriver management...")
            
            # Seleniumæœƒè‡ªå‹•ä¸‹è¼‰é©åˆçš„ChromeDriverç‰ˆæœ¬
            # ä¸éœ€è¦æ‰‹å‹•æŒ‡å®šServiceè·¯å¾‘
            driver = webdriver.Chrome(options=chrome_options)
            print("Success: Chromeç€è¦½å™¨å•Ÿå‹•æˆåŠŸï¼ˆSeleniumè‡ªå‹•ç®¡ç†é©…å‹•ï¼‰")
            
            driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            })
            
            driver.set_page_load_timeout(settings.SCRAPER_TIMEOUT)
            
            # ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨
            try:
                import psutil
                process = psutil.Process()
                memory_before = process.memory_info().rss / 1024 / 1024  # MB
                print(f"Memory: ç€è¦½å™¨å•Ÿå‹•å‰è¨˜æ†¶é«”: {memory_before:.1f} MB")
            except ImportError:
                print("Memory: psutilæœªå®‰è£ï¼Œè·³éè¨˜æ†¶é«”ç›£æ§")
            
            print("æ­£åœ¨è¨ªå•é é¢...")
            driver.get(url)
            
            # æª¢æŸ¥ç€è¦½å™¨æ˜¯å¦ä»ç„¶æ´»è‘—
            try:
                current_url = driver.current_url
                print(f"Success: é é¢è¼‰å…¥æˆåŠŸ: {current_url[:60]}...")
            except Exception as e:
                print(f"Warning: é é¢è¼‰å…¥å¾Œæª¢æŸ¥å¤±æ•—: {e}")
                return None

            # è™•ç†åŒæ„è¦–çª—
            try:
                consent_button_locator = (By.CSS_SELECTOR, "button.consent-button[value='agree'], button[name='agree']")
                consent_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable(consent_button_locator)
                )
                print("Cookie: ç™¼ç¾åŒæ„æŒ‰éˆ•ï¼Œæ­£åœ¨é»æ“Š...")
                driver.execute_script("arguments[0].click();", consent_button)
                time.sleep(random.uniform(1, 2))
            except TimeoutException:
                print("OK: æœªåœ¨10ç§’å…§ç™¼ç¾æˆ–ä¸éœ€é»æ“ŠåŒæ„æŒ‰éˆ•ï¼Œç¹¼çºŒåŸ·è¡Œã€‚")

            # ç­‰å¾…ä¸¦æŠ“å–ä¸»è¦å…§å®¹
            print("Waiting: æ­£åœ¨ç­‰å¾…æ–‡ç« ä¸»è¦å…§å®¹å®¹å™¨...")
            content_container_locator = (By.CSS_SELECTOR, '[data-testid="article-content-wrapper"], div.caas-body, div.atoms-wrapper, div.article-wrap.no-bb')
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located(content_container_locator)
            )
            print("Success: å…§å®¹å®¹å™¨å·²è¼‰å…¥ã€‚")
            
            soup = BeautifulSoup(driver.page_source, "html.parser")
            body = soup.select_one('[data-testid="article-content-wrapper"], div.caas-body, div.atoms-wrapper, div.article-wrap.no-bb')

            if body:
                paragraphs = body.find_all("p")
                if not paragraphs:
                    print(f"Warning: æ‰¾åˆ°å®¹å™¨ {body.get('class')}ï¼Œä½†è£¡é¢æ²’æœ‰ <p> æ¨™ç±¤ã€‚")
                    return None
                
                content_text = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
                
                if content_text:
                    print(f"Success: æˆåŠŸæ“·å–æ–‡ç« å…§æ–‡ï¼Œç´„ {len(content_text)} å­—ã€‚")
                    return content_text
                else:
                    print("Warning: æ‰¾åˆ° <p> æ¨™ç±¤ï¼Œä½†æ²’æœ‰æœ‰æ•ˆæ–‡å­—å…§å®¹ã€‚")
                    return None
            
            print("Error: æœªèƒ½æ‰¾åˆ°ä»»ä½•å·²çŸ¥çš„å…§å®¹å®¹å™¨ (data-testid, caas-body, atoms-wrapper, article-wrap)ã€‚")
            return None

        except TimeoutException as e:
            print(f"Error: æ“·å–å…§æ–‡æ™‚é é¢åŠ è¼‰æˆ–å…ƒç´ ç­‰å¾…è¶…æ™‚ã€‚éŒ¯èª¤è¨Šæ¯: {e.msg}")
            screenshot_path = self.debug_folder / f"TIMEOUT_FAIL_{url.split('/')[-1].replace('.html', '')}.png"
            if driver: 
                driver.save_screenshot(str(screenshot_path))
            print(f"Screenshot: éŒ¯èª¤ç•«é¢å·²æˆªåœ–: {screenshot_path}")
            return None
        except Exception as e:
            print(f"Error: æ“·å–å…§æ–‡æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            screenshot_path = self.debug_folder / f"GENERAL_FAIL_{url.split('/')[-1].replace('.html', '')}.png"
            if driver: 
                driver.save_screenshot(str(screenshot_path))
            print(f"Screenshot: éŒ¯èª¤ç•«é¢å·²æˆªåœ–: {screenshot_path}")
            return None
        finally:
            if driver:
                try:
                    # å¼·åˆ¶æ¸…ç†ç€è¦½å™¨è³‡æº
                    driver.quit()
                    print("Cleanup: ç€è¦½å™¨å·²é—œé–‰ã€‚")
                except Exception as cleanup_error:
                    print(f"Warning: ç€è¦½å™¨æ¸…ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {cleanup_error}")
                    
                # é¡å¤–æ¸…ç†ï¼šå¼·åˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
                
                # è¨˜æ†¶é«”æ¸…ç†å¾Œæª¢æŸ¥
                try:
                    import psutil
                    process = psutil.Process()
                    memory_after = process.memory_info().rss / 1024 / 1024  # MB
                    print(f"Memory: æ¸…ç†å¾Œè¨˜æ†¶é«”: {memory_after:.1f} MB")
                except ImportError:
                    pass
    
    def generate_summary_and_tags(self, title: str, content: str) -> tuple:
        """åŒæ™‚ç”Ÿæˆæ‘˜è¦å’ŒAIæ¨™ç±¤ï¼Œç¯€çœtokenä½¿ç”¨"""
        import os
        import json
        import requests
        
        api_key = os.environ.get('OPENAI_API_KEY')
        if not api_key:
            print("Missing OpenAI API key, using fallback")
            return f"Summary generation failed. Title: {title}", self._generate_fallback_tags(title, content)
        
        # æ ¸å¿ƒæ¨™ç±¤åº«
        core_tags = ["APPLE", "TSMC", "TESLA", "AI_TECH", "CRYPTO"]
        
        prompt = f"""
è«‹ç‚ºä»¥ä¸‹è²¡ç¶“æ–°èåŒæ™‚å®Œæˆå…©å€‹ä»»å‹™ï¼š

æ–°èæ¨™é¡Œï¼š{title}
æ–°èå…§å®¹ï¼š{content[:1500]}

ä»»å‹™1 - ç”Ÿæˆæ‘˜è¦ï¼š
- ä½¿ç”¨ç¹é«”ä¸­æ–‡
- 80-120å­—ä¹‹é–“
- å®¢è§€ä¸­ç«‹ï¼Œçªå‡ºé—œéµè³‡è¨Š
- é©åˆæŠ•è³‡äººå¿«é€Ÿé–±è®€

ä»»å‹™2 - åˆ†é…æ¨™ç±¤ï¼š
å¾ä»¥ä¸‹æ¨™ç±¤åº«é¸æ“‡æœ€ç›¸é—œçš„ï¼ˆæœ€å¤š3å€‹ï¼‰ï¼š
- APPLE: è˜‹æœå…¬å¸ (iPhone, Mac, AAPLè‚¡ç¥¨)
- TSMC: å°ç©é›» (åŠå°é«”, æ™¶åœ“ä»£å·¥)  
- TESLA: ç‰¹æ–¯æ‹‰ (é›»å‹•è»Š, é¦¬æ–¯å…‹)
- AI_TECH: AIç§‘æŠ€ (äººå·¥æ™ºæ…§, AIæ™¶ç‰‡)
- CRYPTO: åŠ å¯†è²¨å¹£ (æ¯”ç‰¹å¹£, å€å¡Šéˆ)

è«‹è¿”å›JSONæ ¼å¼ï¼š
{{
  "summary": "é€™è£¡æ˜¯æ‘˜è¦å…§å®¹...",
  "tags": ["TAG1", "TAG2"],
  "confidence": 0.95
}}
"""
        
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': 'gpt-3.5-turbo',
            'messages': [{'role': 'user', 'content': prompt}],
            'max_tokens': 300,
            'temperature': 0.2
        }
        
        try:
            print("Generating summary and tags with AI...")
            response = requests.post('https://api.openai.com/v1/chat/completions', 
                                   headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                content_result = result['choices'][0]['message']['content']
                
                try:
                    parsed = json.loads(content_result)
                    summary = parsed.get('summary', f"æ‘˜è¦è§£æå¤±æ•—ã€‚åŸæ¨™é¡Œï¼š{title}")
                    tags = parsed.get('tags', [])
                    confidence = parsed.get('confidence', 0)
                    
                    if confidence > 0.7:
                        print(f"AI processing successful: Summary({len(summary)} chars) + Tags{tags}")
                        return summary, tags
                    else:
                        print(f"Low confidence({confidence}), using fallback")
                        return summary, self._generate_fallback_tags(title, content)
                        
                except json.JSONDecodeError as e:
                    print(f"JSON parsing failed: {e}")
                    return f"Summary parsing failed. Title: {title}", self._generate_fallback_tags(title, content)
            else:
                print(f"OpenAI API error: {response.status_code}")
                return f"API error. Title: {title}", self._generate_fallback_tags(title, content)
                
        except Exception as e:
            print(f"Summary and tags generation failed: {e}")
            return f"Processing failed. Title: {title}", self._generate_fallback_tags(title, content)
    
    def _generate_fallback_tags(self, title: str, content: str) -> list:
        """å‚™ç”¨è¦å‰‡å¼æ¨™ç±¤ç”Ÿæˆ"""
        text = f"{title} {content}".lower()
        tags = []
        
        # è¦å‰‡å¼åŒ¹é…
        tag_keywords = {
            "APPLE": ["apple", "iphone", "mac", "aapl", "è˜‹æœ", "åº«å…‹"],
            "TSMC": ["tsmc", "taiwan semiconductor", "å°ç©é›»", "æ™¶åœ“", "åŠå°é«”", "2330"],
            "TESLA": ["tesla", "tsla", "musk", "ç‰¹æ–¯æ‹‰", "é¦¬æ–¯å…‹", "é›»å‹•è»Š"],
            "AI_TECH": ["ai", "artificial intelligence", "chatgpt", "openai", "äººå·¥æ™ºæ…§", "æ©Ÿå™¨å­¸ç¿’", "aiæ™¶ç‰‡"],
            "CRYPTO": ["bitcoin", "cryptocurrency", "blockchain", "æ¯”ç‰¹å¹£", "åŠ å¯†è²¨å¹£", "å€å¡Šéˆ", "btc"]
        }
        
        for tag, keywords in tag_keywords.items():
            if any(keyword in text for keyword in keywords):
                tags.append(tag)
        
        result_tags = tags[:3]  # æœ€å¤š3å€‹æ¨™ç±¤
        print(f"Fallback tags generated: {result_tags}")
        return result_tags
    
    def process_news_for_subscriptions(self) -> bool:
        """
        ä¸»åŸ·è¡Œå‡½å¼ - è™•ç†æ‰€æœ‰è¨‚é–±çš„æ–°èï¼ˆæ‰¹é‡ç‰ˆæœ¬ï¼‰
        æ ¹æ“šæ–°çš„æ¨é€é »ç‡å’Œæ‰¹é‡è™•ç†éœ€æ±‚å„ªåŒ–
        """
        print("Starting intelligent news processing task...")
        
        # ç²å–ç¬¦åˆæ¨é€æ¢ä»¶çš„è¨‚é–±
        eligible_subscriptions = db_manager.get_eligible_subscriptions()
        if not eligible_subscriptions:
            print("No subscriptions eligible for current push time, exiting.")
            return False

        # çˆ¬å–æ–°èåˆ—è¡¨
        news_list = self.scrape_yahoo_finance_list()
        if not news_list:
            print("Could not fetch any news from Yahoo Finance, exiting.")
            return False

        # è™•ç†æ¯å€‹ç¬¦åˆæ¢ä»¶çš„è¨‚é–±
        overall_success = False
        for subscription in eligible_subscriptions:
            print(f"\n--- Processing é–‹å§‹è™•ç†ä½¿ç”¨è€… {subscription['user_id']} çš„è¨‚é–± ---")
            success = self._process_subscription_batch(subscription, news_list)
            if success:
                overall_success = True

        print(f"\nSuccess: æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„è¨‚é–±ä»»å‹™è™•ç†å®Œç•¢ã€‚")
        return overall_success
    
    def _process_subscription_batch(self, subscription: Dict[str, Any], news_list: List[Dict[str, str]]) -> bool:
        """è™•ç†å–®ä¸€è¨‚é–±ä»»å‹™ - æ‰¹é‡æ”¶é›†å’Œæ¨é€ç‰ˆæœ¬"""
        user_id = subscription['user_id']
        frequency_type = subscription.get('push_frequency_type', 'daily')
        keywords = subscription.get("keywords", [])
        max_articles = db_manager.get_max_articles_for_frequency(frequency_type)
        
        print(f"Checking: ç‚ºç”¨æˆ¶ {user_id} æ”¶é›†ç¬¦åˆæ¢ä»¶çš„æ–°è (æœ€å¤š {max_articles} å‰‡)")
        
        # æ”¶é›†ç¬¦åˆæ¢ä»¶çš„æ–°è
        collected_articles = []
        processed_count = 0
        failed_count = 0
        
        for news_item in news_list:
            # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°æœ€å¤§æ•¸é‡
            if len(collected_articles) >= max_articles:
                print(f"Stats: å·²æ”¶é›†åˆ° {max_articles} å‰‡æ–°èï¼Œåœæ­¢æ”¶é›†")
                break
            
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
            if db_manager.is_article_processed(news_item['link']):
                continue
            
            # æª¢æŸ¥é—œéµå­—åŒ¹é…
            if keywords and not any(keyword.lower() in news_item['title'].lower() for keyword in keywords):
                continue
            
            print(f"Found: æ‰¾åˆ°ç¬¦åˆé—œéµå­—çš„æ–‡ç« : {news_item['title'][:60]}...")
            processed_count += 1
            
            # å˜—è©¦è™•ç†æ–‡ç« 
            article_data = self._process_single_article(news_item)
            if article_data:
                collected_articles.append(article_data)
                print(f"Success: æ–‡ç« è™•ç†æˆåŠŸ ({len(collected_articles)}/{max_articles})")
            else:
                failed_count += 1
                print(f"Error: æ–‡ç« è™•ç†å¤±æ•—")
                
            # å¦‚æœå·²æ”¶é›†è¶³å¤ çš„æ–‡ç« ï¼Œæå‰çµæŸ
            if len(collected_articles) >= max_articles:
                break
        
        # æ¨é€çµæœçµ±è¨ˆ
        print(f"\nStats: æ”¶é›†çµæœçµ±è¨ˆ:")
        print(f"  - å˜—è©¦è™•ç†: {processed_count} ç¯‡")
        print(f"  - æˆåŠŸæ”¶é›†: {len(collected_articles)} ç¯‡")
        print(f"  - è™•ç†å¤±æ•—: {failed_count} ç¯‡")
        
        if not collected_articles:
            print(f"[INFO] No suitable articles found for user {user_id}")
            return False
        
        # æ‰¹é‡æ¨é€åˆ° Discord
        print(f"\nSending: é–‹å§‹æ¨é€ {len(collected_articles)} å‰‡æ–°èåˆ° Discord...")
        success, failed_articles = send_batch_to_discord(
            subscription['delivery_target'], 
            collected_articles, 
            subscription
        )
        
        if success:
            # å„²å­˜æˆåŠŸæ¨é€çš„æ–‡ç« ä¸¦è¨˜éŒ„æ­·å²
            successful_articles = [article for article in collected_articles if article not in failed_articles]
            article_ids = []
            
            for article in successful_articles:
                article_id = db_manager.save_new_article(article)
                if article_id:
                    article_ids.append(article_id)
            
            if article_ids:
                # è¨˜éŒ„æ¨é€æ­·å²ï¼ˆæ‰¹é‡è¨˜éŒ„ï¼‰
                batch_success = db_manager.log_push_history(user_id, article_ids)
                if batch_success:
                    print(f"Logging å·²è¨˜éŒ„æ¨é€æ­·å²: {len(article_ids)} ç¯‡æ–‡ç« ")
                
                # æ¨™è¨˜æ¨é€çª—å£ç‚ºå·²å®Œæˆ
                db_manager.mark_push_window_completed(user_id, frequency_type)
                
                # ç™¼é€æ¨é€ç¸½çµæ¶ˆæ¯
                create_push_summary_message(
                    subscription['delivery_target'],
                    len(successful_articles),
                    len(collected_articles),
                    frequency_type
                )
                
                print(f"Completed: ç”¨æˆ¶ {user_id} çš„æ¨é€ä»»å‹™å®Œæˆ: {len(successful_articles)} å‰‡æˆåŠŸ")
                return True
        
        print(f"Error: ç”¨æˆ¶ {user_id} çš„æ¨é€ä»»å‹™å¤±æ•—")
        return False
    
    def _process_single_article(self, news_item: Dict[str, str]) -> Union[Dict[str, Any], None]:
        """è™•ç†å–®ç¯‡æ–‡ç«  - çˆ¬å–å…§å®¹ä¸¦ç”Ÿæˆæ‘˜è¦"""
        try:
            # æŠ“å–å…§å®¹
            content = self.scrape_article_content(news_item['link'])
            if not content:
                return None

            # åŒæ™‚ç”Ÿæˆæ‘˜è¦å’ŒAIæ¨™ç±¤ï¼ˆç¯€çœtokenï¼‰
            summary, tags = self.generate_summary_and_tags(news_item['title'], content)
            if "[æ‘˜è¦ç”Ÿæˆå¤±æ•—" in summary:
                return None

            # è§£ææ–‡ç« ç™¼å¸ƒæ™‚é–“
            published_at = parse_article_publish_time()
            
            # æ§‹å»ºæ–‡ç« æ•¸æ“š
            article_data = {
                'original_url': news_item['link'], 
                'source': 'yahoo_finance', 
                'title': news_item['title'], 
                'summary': summary,
                'tags': tags,  # æ–°å¢AIæ¨™ç±¤
                'published_at': published_at.isoformat()  # è½‰æ›ç‚º ISO æ ¼å¼å­—ç¬¦ä¸²
            }
            
            return article_data
            
        except Exception as e:
            print(f"Error: è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

# Create a global scraper manager instance
scraper_manager = NewsScraperManager() 