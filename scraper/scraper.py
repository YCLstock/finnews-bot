import os
import random
import time
import logging
from pathlib import Path
from typing import Union, List, Dict, Any, Tuple
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

from scraper.scraper_v2 import ScraperV2
from core.config import settings
from core.database import db_manager
from core.utils import parse_article_publish_time
from core.translation_service import translate_title_to_chinese
from core.logger_config import setup_logging
from core.summary_quality_monitor import record_summary_quality, validate_chinese_text, validate_mixed_language_summary, get_quality_monitor

# Initialize logger
logger = logging.getLogger(__name__)

class NewsScraperManager:
    """News scraper manager for FinNews-Bot"""
    
    def __init__(self, use_v2_scraper: bool = False):
        self.debug_folder = self._create_debug_folder()
        self.use_v2_scraper = use_v2_scraper
        if self.use_v2_scraper:
            self.scraper_v2 = ScraperV2()
    
    def _create_debug_folder(self) -> Path:
        path = Path("debug_pages")
        path.mkdir(exist_ok=True)
        return path

    def print_quality_report(self):
        """åˆ—å°æ‘˜è¦å“è³ªå ±å‘Š"""
        try:
            monitor = get_quality_monitor()
            report = monitor.generate_quality_report()
            print("\n" + "="*60)
            print(report)
            print("="*60)
        except Exception as e:
            logger.error(f"ç”¢ç”Ÿå“è³ªå ±å‘Šå¤±æ•—: {e}")
    
    def collect_news_from_topics(self, targets: List[Dict[str, str]], max_articles_to_process: int = None) -> Tuple[bool, Dict[str, int]]:
        """æ–°çš„ä¸»åŸ·è¡Œå‡½å¼ - æ ¹æ“šç›®æ¨™ä¸»é¡Œåˆ—è¡¨æ”¶é›†æ–°è"""
        stats = {
            'total_processed': 0,
            'newly_added': 0,
            'duplicates': 0,
            'failed': 0
        }
        articles_to_save = []
        processed_urls_in_run = set() # æ–°å¢: ç”¨æ–¼è¿½è¹¤æœ¬æ¬¡é‹è¡Œä¸­å·²è™•ç†çš„URL

        for target in targets:
            topic_code = target['topic_code']
            url = target['url']
            logger.info(f"--- é–‹å§‹è™•ç†ä¸»é¡Œ: {topic_code} ---")
            
            news_list = self.scrape_yahoo_finance_list(url)
            if not news_list:
                logger.warning(f"æœªèƒ½å¾ {url} çˆ¬å–åˆ°ä»»ä½•æ–°èåˆ—è¡¨ã€‚")
                continue

            processed_for_topic = 0
            for news_item in news_list:
                # æ–°å¢æª¢æŸ¥: å¦‚æœæ­¤URLåœ¨æœ¬è¼ªé‹è¡Œä¸­å·²è™•ç†ï¼Œå‰‡è·³é
                if news_item['link'] in processed_urls_in_run:
                    logger.info(f"æ–‡ç« åœ¨æœ¬è¼ªå·²è™•ç† (SKIP): {news_item['title'][:50]}...")
                    continue

                if max_articles_to_process is not None and processed_for_topic >= max_articles_to_process:
                    logger.info(f"å·²é”åˆ°ä¸»é¡Œ {topic_code} çš„è™•ç†ä¸Šé™ ({max_articles_to_process} ç¯‡æ–‡ç« )ï¼Œè·³éå‰©é¤˜æ–‡ç« ã€‚")
                    break

                stats['total_processed'] += 1
                
                if db_manager.is_article_processed(news_item['link']):
                    logger.info(f"æ–‡ç« å·²åœ¨è³‡æ–™åº«ä¸­ (SKIP): {news_item['title'][:50]}...")
                    stats['duplicates'] += 1
                    processed_urls_in_run.add(news_item['link']) # ä¹Ÿå°‡å…¶åŠ å…¥ï¼Œé¿å…é‡è¤‡è™•ç†
                    continue

                logger.info(f"æ­£åœ¨è™•ç†æ–°æ–‡ç« : {news_item['title'][:50]}...")
                article_data = self._process_single_article(news_item, topic_code)
                
                if article_data:
                    articles_to_save.append(article_data)
                    processed_urls_in_run.add(news_item['link']) # æ–°å¢: è¨˜éŒ„å·²è™•ç†çš„URL
                    processed_for_topic += 1
                else:
                    stats['failed'] += 1

        if articles_to_save:
            logger.info(f"æº–å‚™å°‡ {len(articles_to_save)} ç¯‡æ–‡ç« é€²è¡Œæ‰¹æ¬¡å„²å­˜...")
            success, count = db_manager.save_new_articles_batch(articles_to_save)
            if success:
                stats['newly_added'] = count
                if count < len(articles_to_save):
                    stats['failed'] += (len(articles_to_save) - count)
            else:
                stats['failed'] += len(articles_to_save)

        # åœ¨å®Œæˆæ™‚åˆ—å°å“è³ªå ±å‘Š
        if articles_to_save:
            logger.info("ğŸ“Š æ­£åœ¨ç”¢ç”Ÿæ‘˜è¦å“è³ªå ±å‘Š...")
            self.print_quality_report()
        
        return True, stats

    def scrape_yahoo_finance_list(self, url: str) -> List[Dict[str, str]]:
        """ä½¿ç”¨ requests çˆ¬å– Yahoo Finance çš„æ–°èåˆ—è¡¨é """
        logger.info(f"æ­£åœ¨å¾ {url} çˆ¬å–æ–°èåˆ—è¡¨...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select("li.stream-item")
            result = []
            seen_links = set()
            
            for item in items:
                a_tag = item.select_one('a[href*="/news/"]')
                title_tag = item.select_one('h3')
                if a_tag and title_tag:
                    link = requests.compat.urljoin(url, a_tag['href'])
                    if link not in seen_links:
                        seen_links.add(link)
                        result.append({
                            'title': title_tag.get_text(strip=True), 
                            'link': link
                        })
            
            logger.info(f"æˆåŠŸçˆ¬å–åˆ° {len(result)} å‰‡æ–°èæ¨™é¡Œã€‚")
            return result
        except Exception as e:
            logger.exception(f"çˆ¬å–æ–°èåˆ—è¡¨å¤±æ•—: {url}")
            return []
    
    def _process_single_article(self, news_item: Dict[str, str], topic_code: str) -> Union[Dict[str, Any], None]:
        """è™•ç†å–®ç¯‡æ–‡ç«  - çˆ¬å–å…§å®¹ã€ç”Ÿæˆæ‘˜è¦/æ¨™ç±¤ï¼Œä¸¦é™„ä¸Šä¾†æºä¸»é¡Œ"""
        try:
            content = self.scrape_article_content(news_item['link'])
            if not content:
                return None

            summary, tags = self.generate_summary_and_tags(news_item['title'], content)
            if "[æ‘˜è¦ç”Ÿæˆå¤±æ•—" in summary:
                return None

            # ğŸŒ æ–°å¢ç¿»è­¯åŠŸèƒ½ï¼šç¿»è­¯æ¨™é¡Œç‚ºä¸­æ–‡
            translated_title = None
            try:
                logger.info(f"ğŸŒ é–‹å§‹ç¿»è­¯æ¨™é¡Œ: {news_item['title'][:50]}...")
                translated_title = translate_title_to_chinese(news_item['title'])
                
                if translated_title:
                    logger.info(f"âœ… ç¿»è­¯æˆåŠŸ: {translated_title[:50]}...")
                else:
                    logger.info("â„¹ï¸ æ¨™é¡Œç„¡éœ€ç¿»è­¯æˆ–ç¿»è­¯å¤±æ•—ï¼Œå°‡å„²å­˜åŸæ¨™é¡Œ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ç¿»è­¯éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼Œä½†ä¸å½±éŸ¿æ–‡ç« è™•ç†: {e}")
                translated_title = None

            published_at = parse_article_publish_time()
            
            logger.debug(f"Inside _process_single_article - Type of news_item['link']: {type(news_item['link'])}")
            logger.debug(f"Inside _process_single_article - Type of tags: {type(tags)}")

            article_data = {
                'original_url': news_item['link'], 
                'source': 'yahoo_finance', 
                'title': news_item['title'], 
                'summary': summary,
                'translated_title': translated_title,  # ğŸŒ æ–°å¢ç¿»è­¯æ¨™é¡Œæ¬„ä½
                'tags': tags,
                'topics': [topic_code],
                'published_at': published_at.isoformat()
            }
            
            return article_data
            
        except Exception as e:
            logger.exception(f"è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {news_item.get('link', 'N/A')}")
            return None

    def scrape_article_content(self, url: str) -> Union[str, None]:
        if self.use_v2_scraper:
            logger.info(f"ä½¿ç”¨ ScraperV2 æŠ“å–: {url[:70]}...")
            result = self.scraper_v2._scrape_single_article(url)
            if result['success']:
                return result['content']
            else:
                logger.error(f"ScraperV2 æŠ“å–å¤±æ•—: {result['error']} for url: {url}")
                return None
        else:
            logger.info(f"ä½¿ç”¨ Selenium æ­£åœ¨å•Ÿå‹•ç€è¦½å™¨æŠ“å–: {url[:70]}...")
            
            chrome_options = Options()
            chrome_options.add_argument("--headless=new")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1024,768")
            chrome_options.add_argument("user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36")

            driver = None
            try:
                driver = webdriver.Chrome(options=chrome_options)
                driver.set_page_load_timeout(settings.SCRAPER_TIMEOUT)
                driver.get(url)

                try:
                    consent_button_locator = (By.CSS_SELECTOR, "button.consent-button[value='agree'], button[name='agree']")
                    consent_button = WebDriverWait(driver, 10).until(
                        EC.element_to_be_clickable(consent_button_locator)
                    )
                    driver.execute_script("arguments[0].click();", consent_button)
                    time.sleep(random.uniform(1, 2))
                except TimeoutException:
                    logger.debug("No consent button found.")
                    pass

                content_container_locator = (By.CSS_SELECTOR, '[data-testid="article-content-wrapper"], div.caas-body')
                WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located(content_container_locator)
                )
                
                soup = BeautifulSoup(driver.page_source, "html.parser")
                body = soup.select_one('[data-testid="article-content-wrapper"], div.caas-body')

                if body:
                    content_text = "\n".join(p.get_text(strip=True) for p in body.find_all("p") if p.get_text(strip=True))
                    if content_text:
                        logger.info(f"æˆåŠŸæ“·å–æ–‡ç« å…§æ–‡ï¼Œç´„ {len(content_text)} å­—ã€‚")
                        return content_text
                
                logger.warning(f"æœªæ‰¾åˆ°ä¸»è¦å…§æ–‡å®¹å™¨ for url: {url}")
                return None

            except Exception as e:
                logger.exception(f"ä½¿ç”¨ Selenium æ“·å–å…§æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {url}")
                return None
            finally:
                if driver:
                    driver.quit()
    
    def _validate_chinese_summary(self, summary: str) -> tuple[bool, float, float, bool, dict]:
        """é©—è­‰æ··åˆèªè¨€æ‘˜è¦çš„å“è³ª"""
        is_valid, chinese_ratio, has_forbidden_words, analysis = validate_mixed_language_summary(summary)
        
        # è¨ˆç®—å“è³ªåˆ†æ•¸
        quality_score = chinese_ratio
        
        # å¦‚æœåŒ…å«ç¦ç”¨è©å½™ï¼Œå¤§å¹…é™ä½åˆ†æ•¸
        if has_forbidden_words:
            quality_score *= 0.3  # åš´é‡æ‡²ç½°ç¦ç”¨è©å½™
        
        # å¦‚æœæœ‰æœªçŸ¥è‹±æ–‡è©å½™ï¼Œè¼•å¾®é™ä½åˆ†æ•¸
        if analysis.get('unknown_words'):
            unknown_count = len(analysis['unknown_words'])
            quality_score *= max(0.7, 1.0 - unknown_count * 0.1)  # æ¯å€‹æœªçŸ¥è©é™ä½10%
        
        # å¦‚æœåŒ…å«å…è¨±çš„å°ˆæ¥­è¡“èªï¼Œè¼•å¾®æé«˜åˆ†æ•¸ï¼ˆé¼“å‹µå°ˆæ¥­æ€§ï¼‰
        if analysis.get('allowed_words'):
            allowed_count = len(analysis['allowed_words'])
            quality_score *= min(1.0, 1.0 + allowed_count * 0.02)  # æ¯å€‹å°ˆæ¥­è¡“èªåŠ åˆ†2%
        
        logger.debug(f"æ‘˜è¦èªè¨€é©—è­‰: ä¸­æ–‡æ¯”ä¾‹={chinese_ratio:.2f}, ç¦ç”¨è©å½™={has_forbidden_words}, æœ‰æ•ˆ={is_valid}, åˆ†æ•¸={quality_score:.2f}")
        logger.debug(f"è©³ç´°åˆ†æ: {analysis}")
        
        return is_valid, quality_score, chinese_ratio, has_forbidden_words, analysis

    def generate_summary_and_tags(self, title: str, content: str) -> tuple:
        """åŒæ™‚ç”Ÿæˆæ‘˜è¦å’ŒAIæ¨™ç±¤ - å¢å¼·ç‰ˆï¼ˆç¢ºä¿ä¸­æ–‡è¼¸å‡ºï¼‰"""
        api_key = os.environ.get('OPENAI_API_KEY')
        if not api_key:
            logger.error("OPENAI_API_KEY not found, cannot generate summary and tags.")
            return f"Summary generation failed. Title: {title}", []
        
        # ä½¿ç”¨çµ±ä¸€æ¨™ç±¤ç®¡ç†ç³»çµ±
        try:
            from scripts.dynamic_tags import get_tags_for_scraper
            core_tags = get_tags_for_scraper()
        except Exception as e:
            logger.warning(f"Failed to load dynamic tags, using fallback: {e}")
            # é™ç´šåˆ°ç¡¬ç·¨ç¢¼æ¨™ç±¤ä½œç‚ºå‚™ç”¨
            core_tags = [
                "APPLE", "TSMC", "TESLA", "AI_TECH", "CRYPTO",
                "STOCK_MARKET", "ECONOMIES", "LATEST", "EARNINGS", 
                "TECH", "ELECTRIC_VEHICLES", "FEDERAL_RESERVE",
                "HOUSING", "ENERGY", "HEALTHCARE", "FINANCE",
                "TARIFFS", "TRADE", "COMMODITIES", "BONDS"
            ]
        
        # æ–°çš„å¹³è¡¡ç­–ç•¥ Prompt - è‹±æ–‡æŒ‡ä»¤æ¡†æ¶é…ä¸­æ–‡å…§å®¹
        prompt = f'''You are a professional Taiwan financial news summarizer. Please complete the following tasks for this financial news:

News Title: {title}
News Content: {content[:1500]}

TASK 1 - Generate Summary:
- LANGUAGE: Must use Traditional Chinese (ç¹é«”ä¸­æ–‡) as primary language
- LENGTH: 80-120 characters
- CONTENT: Objective, neutral, highlight key financial data and insights
- PROPER NOUNS: For English company names, person names, or technical terms, you may keep them in English if they are commonly used in Taiwan financial media (e.g., Apple, Tesla, TSMC, GDP, AI, CEO, IPO)
- AVOID: English grammatical words (the, and, with, but, in, on, at, etc.)
- STYLE: Natural mix that Taiwanese readers would expect in financial news

TASK 2 - Assign Tags:
Select most relevant tags (max 3) from: {core_tags}

OUTPUT FORMAT (strictly follow JSON):
{{
  "summary": "ä¸»è¦ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œå¯é©ç•¶ä¿ç•™å°ˆæ¥­è‹±æ–‡è¡“èªå¦‚Appleã€GDPç­‰",
  "tags": ["TAG1", "TAG2"],
  "confidence": 0.9
}}

CRITICAL: The summary should be primarily Traditional Chinese with appropriate English technical terms/company names where natural for Taiwan financial readers. Avoid English grammatical words completely.'''
        
        headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}
        # å„ªåŒ–æ¨¡å‹åƒæ•¸ï¼šæé«˜ temperature å¢åŠ å‰µé€ æ€§ï¼Œå¢åŠ  max_tokens
        data = {'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': prompt}], 'max_tokens': 350, 'temperature': 0.4}
        
        import json
        import time
        max_retries = 2
        start_time = time.time()
        
        for attempt in range(max_retries + 1):
            try:
                logger.info(f"ğŸ¤– å˜—è©¦ç”Ÿæˆæ‘˜è¦ (ç¬¬ {attempt + 1}/{max_retries + 1} æ¬¡): {title[:50]}...")
                
                response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, json=data, timeout=30)
                if response.status_code == 200:
                    result = response.json()['choices'][0]['message']['content']
                    
                    try:
                        parsed = json.loads(result)
                        summary = parsed.get('summary', f"æ‘˜è¦è§£æå¤±æ•—ã€‚åŸæ¨™é¡Œï¼š{title}")
                        tags = parsed.get('tags', [])
                        
                        if not isinstance(tags, list):
                            tags = []
                        
                        # é©—è­‰æ··åˆèªè¨€æ‘˜è¦å“è³ª
                        is_valid_summary, quality_score, chinese_ratio, has_forbidden_words, analysis = self._validate_chinese_summary(summary)
                        generation_time = time.time() - start_time
                        
                        # è¨˜éŒ„å“è³ªæŒ‡æ¨™ï¼ˆåŒ…å«è©³ç´°åˆ†æï¼‰
                        record_summary_quality(
                            title=title,
                            summary=summary,
                            chinese_ratio=chinese_ratio,
                            has_english_words=has_forbidden_words,  # é€™è£¡æŒ‡çš„æ˜¯ç¦ç”¨è©å½™
                            is_valid=is_valid_summary,
                            quality_score=quality_score,
                            attempt_count=attempt + 1,
                            generation_time=generation_time,
                            success=is_valid_summary,
                            detailed_analysis=analysis
                        )
                        
                        if is_valid_summary:
                            allowed_terms = ', '.join(analysis.get('allowed_words', [])) if analysis.get('allowed_words') else 'ç„¡'
                            logger.info(f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸ (å“è³ª: {quality_score:.2f}, å°ˆæ¥­è¡“èª: {allowed_terms}): {summary[:50]}...")
                            return summary, tags
                        else:
                            # æä¾›è©³ç´°çš„å¤±æ•—åŸå› 
                            failure_reasons = []
                            if chinese_ratio < 0.6:
                                failure_reasons.append(f"ä¸­æ–‡æ¯”ä¾‹éä½({chinese_ratio:.1%})")
                            if has_forbidden_words:
                                forbidden_list = ', '.join(analysis.get('forbidden_words', []))
                                failure_reasons.append(f"åŒ…å«ç¦ç”¨è©å½™({forbidden_list})")
                            
                            reason_text = '; '.join(failure_reasons)
                            logger.warning(f"âš ï¸ æ‘˜è¦ä¸åˆæ ¼ (åˆ†æ•¸: {quality_score:.2f}): {reason_text}")
                            
                            if attempt < max_retries:
                                logger.info(f"ğŸ”„ å°‡é‡æ–°å˜—è©¦ç”Ÿæˆæ”¹é€²ç‰ˆæ‘˜è¦...")
                                # æ ¹æ“šå…·é«”å•é¡ŒåŠ å¼·æç¤º
                                retry_hints = []
                                if chinese_ratio < 0.6:
                                    retry_hints.append("Use MORE Traditional Chinese characters")
                                if has_forbidden_words:
                                    forbidden_list = ', '.join(analysis.get('forbidden_words', []))
                                    retry_hints.append(f"AVOID these English words: {forbidden_list}")
                                
                                retry_prompt = prompt + f"\n\n**RETRY INSTRUCTIONS**: {'; '.join(retry_hints)}. Focus on natural Traditional Chinese with appropriate technical terms only."
                                data['messages'][0]['content'] = retry_prompt
                                continue
                            else:
                                logger.error(f"âŒ å¤šæ¬¡é‡è©¦å¾Œä»ç„¡æ³•ç”Ÿæˆåˆæ ¼æ‘˜è¦: {reason_text}")
                                
                                # è¨˜éŒ„æœ€çµ‚å¤±æ•—ï¼ˆå·²åœ¨ä¸Šé¢è¨˜éŒ„éï¼‰
                                return f"æ‘˜è¦å“è³ªé©—è­‰å¤±æ•—: {reason_text}ã€‚åŸæ¨™é¡Œï¼š{title}", tags
                                
                    except json.JSONDecodeError as e:
                        logger.error(f"JSON è§£æå¤±æ•— (ç¬¬ {attempt + 1} æ¬¡)ã€‚åŸå§‹å›æ‡‰: {result[:200]}...")
                        if attempt >= max_retries:
                            # è¨˜éŒ„ JSON è§£æå¤±æ•—
                            record_summary_quality(
                                title=title,
                                summary="",
                                chinese_ratio=0.0,
                                has_english_words=False,
                                is_valid=False,
                                quality_score=0.0,
                                attempt_count=attempt + 1,
                                generation_time=time.time() - start_time,
                                success=False,
                                error_message=f"JSONè§£æå¤±æ•—: {str(e)}"
                            )
                            return f"JSON è§£æå¤±æ•—ã€‚åŸæ¨™é¡Œï¼š{title}", []
                        continue
                        
                else:
                    logger.error(f"OpenAI API éŒ¯èª¤ (ç¬¬ {attempt + 1} æ¬¡)ã€‚ç‹€æ…‹ç¢¼: {response.status_code}, å›æ‡‰: {response.text[:200]}")
                    if attempt >= max_retries:
                        # è¨˜éŒ„ API éŒ¯èª¤
                        record_summary_quality(
                            title=title,
                            summary="",
                            chinese_ratio=0.0,
                            has_english_words=False,
                            is_valid=False,
                            quality_score=0.0,
                            attempt_count=attempt + 1,
                            generation_time=time.time() - start_time,
                            success=False,
                            error_message=f"APIéŒ¯èª¤: {response.status_code}"
                        )
                        return f"API error. Title: {title}", []
                    continue
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"è«‹æ±‚ OpenAI API å¤±æ•— (ç¬¬ {attempt + 1} æ¬¡): {e}")
                if attempt >= max_retries:
                    # è¨˜éŒ„ç¶²è·¯éŒ¯èª¤
                    record_summary_quality(
                        title=title,
                        summary="",
                        chinese_ratio=0.0,
                        has_english_words=False,
                        is_valid=False,
                        quality_score=0.0,
                        attempt_count=attempt + 1,
                        generation_time=time.time() - start_time,
                        success=False,
                        error_message=f"ç¶²è·¯è«‹æ±‚å¤±æ•—: {str(e)}"
                    )
                    return f"è«‹æ±‚å¤±æ•—ã€‚åŸæ¨™é¡Œï¼š{title}", []
                continue
                
            except Exception as e:
                logger.error(f"è™•ç† OpenAI å›æ‡‰æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ (ç¬¬ {attempt + 1} æ¬¡): {e}")
                if attempt >= max_retries:
                    # è¨˜éŒ„æœªçŸ¥éŒ¯èª¤
                    record_summary_quality(
                        title=title,
                        summary="",
                        chinese_ratio=0.0,
                        has_english_words=False,
                        is_valid=False,
                        quality_score=0.0,
                        attempt_count=attempt + 1,
                        generation_time=time.time() - start_time,
                        success=False,
                        error_message=f"æœªçŸ¥éŒ¯èª¤: {str(e)}"
                    )
                    return f"è™•ç†å¤±æ•—ã€‚åŸæ¨™é¡Œï¼š{title}", []
                continue
        
        # å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†
        logger.error(f"âŒ ç¶“é {max_retries + 1} æ¬¡å˜—è©¦å¾Œï¼Œæ‘˜è¦ç”Ÿæˆå®Œå…¨å¤±æ•—")
        
        # è¨˜éŒ„æœ€çµ‚å¤±æ•—
        record_summary_quality(
            title=title,
            summary="",
            chinese_ratio=0.0,
            has_english_words=False,
            is_valid=False,
            quality_score=0.0,
            attempt_count=max_retries + 1,
            generation_time=time.time() - start_time,
            success=False,
            error_message="æ‰€æœ‰é‡è©¦å°è©¦å‡å¤±æ•—"
        )
        
        return f"[æ‘˜è¦ç”Ÿæˆå¤±æ•—] åŸæ¨™é¡Œï¼š{title}", []

# Setup logging
setup_logging()

# Create a global scraper manager instance
scraper_manager = NewsScraperManager()