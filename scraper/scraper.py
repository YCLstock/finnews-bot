import os
import random
import time
from pathlib import Path
from typing import Union, List, Dict, Any
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager

from core.config import settings
from core.database import db_manager
from core.utils import generate_summary_optimized, send_batch_to_discord, create_push_summary_message, parse_article_publish_time

class NewsScraperManager:
    """News scraper manager for FinNews-Bot"""
    
    def __init__(self):
        self.debug_folder = self._create_debug_folder()
    
    def _create_debug_folder(self) -> Path:
        """å‰µå»ºä¸€å€‹ç”¨æ–¼å­˜æ”¾é™¤éŒ¯æˆªåœ–çš„è³‡æ–™å¤¾"""
        path = Path("debug_pages")
        path.mkdir(exist_ok=True)
        return path
    
    def scrape_yahoo_finance_list(self, url: str = None) -> List[Dict[str, str]]:
        """ä½¿ç”¨ requests çˆ¬å– Yahoo Finance çš„æ–°èåˆ—è¡¨é """
        if url is None:
            url = settings.YAHOO_FINANCE_URL
        
        print(f"ğŸ“° æ­£åœ¨å¾ {url} çˆ¬å–æ–°èåˆ—è¡¨...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select("li.stream-item")
            result = []
            seen_links = set()
            
            for item in items:
                a_tag = item.select_one('a[href*="/news/"]')
                title_tag = item.select_one('h3')
                if a_tag and title_tag:
                    link = requests.compat.urljoin(url, a_tag['href'])
                    if link not in seen_links:
                        seen_links.add(link)
                        result.append({
                            'title': title_tag.get_text(strip=True), 
                            'link': link
                        })
            
            print(f"ğŸ‘ æˆåŠŸçˆ¬å–åˆ° {len(result)} å‰‡æ–°èæ¨™é¡Œã€‚")
            return result
        except Exception as e:
            print(f"âŒ çˆ¬å–æ–°èåˆ—è¡¨å¤±æ•—: {e}")
            return []
    
    def scrape_article_content(self, url: str) -> Union[str, None]:
        """
        ä½¿ç”¨ Selenium çˆ¬å–å–®ç¯‡æ–°èçš„å…§æ–‡
        - å¼·åŒ–å½è£ä»¥æ‡‰å°åçˆ¬èŸ²æ©Ÿåˆ¶
        - ä½¿ç”¨è¤‡åˆé¸æ“‡å™¨ä»¥æ‡‰å°å¤šç¨®é é¢ç‰ˆé¢
        """
        print(f"ğŸ¦¾ [Selenium] æ­£åœ¨å•Ÿå‹•ç€è¦½å™¨æŠ“å–å®Œæ•´ URL: {url}")
        
        # å°å…¥å¿…è¦æ¨¡çµ„
        import os
        import shutil
        import stat
        import platform
        from webdriver_manager.chrome import ChromeDriverManager
        
        chrome_options = Options()
        
        # è¨˜æ†¶é«”å„ªåŒ–çš„Chromeè¨­å®š
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        
        # è¨˜æ†¶é«”é™åˆ¶è¨­å®š
        chrome_options.add_argument("--memory-pressure-off")
        chrome_options.add_argument("--max_old_space_size=1024")  # é™åˆ¶JSå †è¨˜æ†¶é«”ç‚º1GB
        chrome_options.add_argument("--aggressive-cache-discard")
        chrome_options.add_argument("--disable-background-timer-throttling")
        chrome_options.add_argument("--disable-renderer-backgrounding")
        chrome_options.add_argument("--disable-backgrounding-occluded-windows")
        chrome_options.add_argument("--disable-background-networking")
        
        # åŠŸèƒ½ç¦ç”¨ï¼ˆæ¸›å°‘è¨˜æ†¶é«”ä½”ç”¨ï¼‰
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument("--disable-images")  # ç¦ç”¨åœ–ç‰‡è¼‰å…¥
        chrome_options.add_argument("--disable-javascript")  # ç¦ç”¨JavaScriptï¼ˆæ–°èå…§å®¹é€šå¸¸åœ¨HTMLä¸­ï¼‰
        chrome_options.add_argument("--disable-css")  # ç¦ç”¨CSS
        chrome_options.add_argument("--disable-features=TranslateUI,VizDisplayCompositor")
        chrome_options.add_argument("--disable-ipc-flooding-protection")
        
        # çª—å£å’Œæ¸²æŸ“è¨­å®š
        chrome_options.add_argument("--window-size=1024,768")  # æ¸›å°çª—å£å°ºå¯¸
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--ignore-certificate-errors")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        
        # GitHub Actionsç‰¹æ®Šè¨­å®š
        if os.environ.get('GITHUB_ACTIONS'):
            chrome_options.add_argument("--single-process")  # å¼·åˆ¶å–®é€²ç¨‹æ¨¡å¼
            chrome_options.add_argument("--disable-software-rasterizer")
            chrome_options.add_argument("--disable-background-networking")
            print("ğŸ”§ GitHub Actionsç’°å¢ƒï¼šå•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼")
            
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # è¨­å®šUser-Agent
        chrome_options.add_argument("user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36")

        driver = None
        try:
            # æ¸…é™¤èˆŠçš„driverç·©å­˜
            cache_path = os.path.expanduser("~/.wdm")
            if os.path.exists(cache_path):
                try:
                    shutil.rmtree(cache_path)
                    print("ğŸ§¹ å·²æ¸…é™¤èˆŠç‰ˆChromeDriverç·©å­˜")
                except:
                    pass
            
            # ä¸‹è¼‰ä¸¦ä¿®å¾©ChromeDriverè·¯å¾‘
            driver_path = ChromeDriverManager().install()
            
            # Linuxç’°å¢ƒç‰¹æ®Šè™•ç†
            if platform.system() == "Linux":
                print(f"ğŸ” æª¢æŸ¥ChromeDriverè·¯å¾‘: {driver_path}")
                
                # æª¢æŸ¥æ˜¯å¦æŒ‡å‘éŒ¯èª¤çš„æ–‡ä»¶æˆ–ä¸å¯åŸ·è¡Œ
                needs_fix = (
                    "THIRD_PARTY_NOTICES" in driver_path or 
                    not os.path.isfile(driver_path) or
                    not os.access(driver_path, os.X_OK)
                )
                
                if needs_fix:
                    print("âš ï¸ ChromeDriverè·¯å¾‘éœ€è¦ä¿®å¾©")
                    # å˜—è©¦æ‰¾åˆ°æ­£ç¢ºçš„chromedriveråŸ·è¡Œæª”
                    driver_dir = os.path.dirname(driver_path)
                    base_dir = os.path.dirname(driver_dir)
                    
                    possible_paths = [
                        # åœ¨åŒç›®éŒ„ä¸‹å°‹æ‰¾
                        os.path.join(driver_dir, "chromedriver"),
                        # åœ¨chromedriver-linux64å­ç›®éŒ„ä¸­å°‹æ‰¾
                        os.path.join(driver_dir, "chromedriver-linux64", "chromedriver"),
                        # åœ¨çˆ¶ç›®éŒ„ä¸­å°‹æ‰¾
                        os.path.join(base_dir, "chromedriver"),
                        # éæ­¸æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½ä½ç½®
                        os.path.join(base_dir, "chromedriver-linux64", "chromedriver"),
                    ]
                    
                    # å¦‚æœä¸Šè¿°è·¯å¾‘éƒ½ä¸å­˜åœ¨ï¼Œé€²è¡Œæ·±åº¦æœç´¢
                    try:
                        for root, dirs, files in os.walk(base_dir):
                            if "chromedriver" in files:
                                candidate = os.path.join(root, "chromedriver")
                                if os.access(candidate, os.X_OK):
                                    possible_paths.append(candidate)
                    except Exception as e:
                        print(f"æ·±åº¦æœç´¢å¤±æ•—: {e}")
                    
                    # å˜—è©¦æ¯å€‹å¯èƒ½çš„è·¯å¾‘
                    fixed = False
                    for path in possible_paths:
                        if os.path.isfile(path) and os.access(path, os.X_OK):
                            driver_path = path
                            print(f"ğŸ”§ ä¿®æ­£ChromeDriverè·¯å¾‘: {driver_path}")
                            fixed = True
                            break
                    
                    if not fixed:
                        print("âŒ ç„¡æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ChromeDriveråŸ·è¡Œæª”")
                        print(f"ğŸ” å˜—è©¦éçš„è·¯å¾‘: {possible_paths}")
                        return None
                
                # ç¢ºä¿åŸ·è¡Œæ¬Šé™
                if os.path.isfile(driver_path):
                    try:
                        current_permissions = os.stat(driver_path).st_mode
                        os.chmod(driver_path, current_permissions | stat.S_IEXEC)
                        print("âœ… å·²è¨­ç½®ChromeDriveråŸ·è¡Œæ¬Šé™")
                    except Exception as e:
                        print(f"âš ï¸ è¨­ç½®åŸ·è¡Œæ¬Šé™å¤±æ•—: {e}")
                        
                # æœ€çµ‚é©—è­‰
                if not os.access(driver_path, os.X_OK):
                    print("âŒ ChromeDriverä»ç„¶ä¸å¯åŸ·è¡Œ")
                    return None
                else:
                    print(f"âœ… ChromeDriveré©—è­‰é€šé: {driver_path}")
            
            service = Service(driver_path)
            driver = webdriver.Chrome(service=service, options=chrome_options)
            
            driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            })
            
            driver.set_page_load_timeout(settings.SCRAPER_TIMEOUT)
            
            # ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨
            try:
                import psutil
                process = psutil.Process()
                memory_before = process.memory_info().rss / 1024 / 1024  # MB
                print(f"ğŸ’¾ ç€è¦½å™¨å•Ÿå‹•å‰è¨˜æ†¶é«”: {memory_before:.1f} MB")
            except ImportError:
                print("ğŸ’¾ psutilæœªå®‰è£ï¼Œè·³éè¨˜æ†¶é«”ç›£æ§")
            
            print("æ­£åœ¨è¨ªå•é é¢...")
            driver.get(url)
            
            # æª¢æŸ¥ç€è¦½å™¨æ˜¯å¦ä»ç„¶æ´»è‘—
            try:
                current_url = driver.current_url
                print(f"âœ… é é¢è¼‰å…¥æˆåŠŸ: {current_url[:60]}...")
            except Exception as e:
                print(f"âš ï¸ é é¢è¼‰å…¥å¾Œæª¢æŸ¥å¤±æ•—: {e}")
                return None

            # è™•ç†åŒæ„è¦–çª—
            try:
                consent_button_locator = (By.CSS_SELECTOR, "button.consent-button[value='agree'], button[name='agree']")
                consent_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable(consent_button_locator)
                )
                print("ğŸª ç™¼ç¾åŒæ„æŒ‰éˆ•ï¼Œæ­£åœ¨é»æ“Š...")
                driver.execute_script("arguments[0].click();", consent_button)
                time.sleep(random.uniform(1, 2))
            except TimeoutException:
                print("ğŸ‘ æœªåœ¨10ç§’å…§ç™¼ç¾æˆ–ä¸éœ€é»æ“ŠåŒæ„æŒ‰éˆ•ï¼Œç¹¼çºŒåŸ·è¡Œã€‚")

            # ç­‰å¾…ä¸¦æŠ“å–ä¸»è¦å…§å®¹
            print("â³ æ­£åœ¨ç­‰å¾…æ–‡ç« ä¸»è¦å…§å®¹å®¹å™¨...")
            content_container_locator = (By.CSS_SELECTOR, '[data-testid="article-content-wrapper"], div.caas-body, div.atoms-wrapper, div.article-wrap.no-bb')
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located(content_container_locator)
            )
            print("âœ… å…§å®¹å®¹å™¨å·²è¼‰å…¥ã€‚")
            
            soup = BeautifulSoup(driver.page_source, "html.parser")
            body = soup.select_one('[data-testid="article-content-wrapper"], div.caas-body, div.atoms-wrapper, div.article-wrap.no-bb')

            if body:
                paragraphs = body.find_all("p")
                if not paragraphs:
                    print(f"âš ï¸ æ‰¾åˆ°å®¹å™¨ {body.get('class')}ï¼Œä½†è£¡é¢æ²’æœ‰ <p> æ¨™ç±¤ã€‚")
                    return None
                
                content_text = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
                
                if content_text:
                    print(f"âœ… æˆåŠŸæ“·å–æ–‡ç« å…§æ–‡ï¼Œç´„ {len(content_text)} å­—ã€‚")
                    return content_text
                else:
                    print("âš ï¸ æ‰¾åˆ° <p> æ¨™ç±¤ï¼Œä½†æ²’æœ‰æœ‰æ•ˆæ–‡å­—å…§å®¹ã€‚")
                    return None
            
            print("âŒ æœªèƒ½æ‰¾åˆ°ä»»ä½•å·²çŸ¥çš„å…§å®¹å®¹å™¨ (data-testid, caas-body, atoms-wrapper, article-wrap)ã€‚")
            return None

        except TimeoutException as e:
            print(f"âŒ æ“·å–å…§æ–‡æ™‚é é¢åŠ è¼‰æˆ–å…ƒç´ ç­‰å¾…è¶…æ™‚ã€‚éŒ¯èª¤è¨Šæ¯: {e.msg}")
            screenshot_path = self.debug_folder / f"TIMEOUT_FAIL_{url.split('/')[-1].replace('.html', '')}.png"
            if driver: 
                driver.save_screenshot(str(screenshot_path))
            print(f"ğŸ“¸ éŒ¯èª¤ç•«é¢å·²æˆªåœ–: {screenshot_path}")
            return None
        except Exception as e:
            print(f"âŒ æ“·å–å…§æ–‡æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            screenshot_path = self.debug_folder / f"GENERAL_FAIL_{url.split('/')[-1].replace('.html', '')}.png"
            if driver: 
                driver.save_screenshot(str(screenshot_path))
            print(f"ğŸ“¸ éŒ¯èª¤ç•«é¢å·²æˆªåœ–: {screenshot_path}")
            return None
        finally:
            if driver:
                try:
                    # å¼·åˆ¶æ¸…ç†ç€è¦½å™¨è³‡æº
                    driver.quit()
                    print("ğŸ§¹ ç€è¦½å™¨å·²é—œé–‰ã€‚")
                except Exception as cleanup_error:
                    print(f"âš ï¸ ç€è¦½å™¨æ¸…ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {cleanup_error}")
                    
                # é¡å¤–æ¸…ç†ï¼šå¼·åˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()
                
                # è¨˜æ†¶é«”æ¸…ç†å¾Œæª¢æŸ¥
                try:
                    import psutil
                    process = psutil.Process()
                    memory_after = process.memory_info().rss / 1024 / 1024  # MB
                    print(f"ğŸ’¾ æ¸…ç†å¾Œè¨˜æ†¶é«”: {memory_after:.1f} MB")
                except ImportError:
                    pass
    
    def process_news_for_subscriptions(self) -> bool:
        """
        ä¸»åŸ·è¡Œå‡½å¼ - è™•ç†æ‰€æœ‰è¨‚é–±çš„æ–°èï¼ˆæ‰¹é‡ç‰ˆæœ¬ï¼‰
        æ ¹æ“šæ–°çš„æ¨é€é »ç‡å’Œæ‰¹é‡è™•ç†éœ€æ±‚å„ªåŒ–
        """
        print("ğŸš€ é–‹å§‹åŸ·è¡Œæ™ºèƒ½æ–°èè™•ç†ä»»å‹™...")
        
        # ç²å–ç¬¦åˆæ¨é€æ¢ä»¶çš„è¨‚é–±
        eligible_subscriptions = db_manager.get_eligible_subscriptions()
        if not eligible_subscriptions:
            print("ğŸŸ¡ ç›®å‰æ²’æœ‰ç¬¦åˆæ¨é€æ™‚é–“çš„è¨‚é–±ä»»å‹™ï¼Œç¨‹å¼çµæŸã€‚")
            return False

        # çˆ¬å–æ–°èåˆ—è¡¨
        news_list = self.scrape_yahoo_finance_list()
        if not news_list:
            print("ğŸŸ¡ æœªèƒ½å¾ Yahoo Finance çˆ¬å–åˆ°ä»»ä½•æ–°èï¼Œç¨‹å¼çµæŸã€‚")
            return False

        # è™•ç†æ¯å€‹ç¬¦åˆæ¢ä»¶çš„è¨‚é–±
        overall_success = False
        for subscription in eligible_subscriptions:
            print(f"\n--- âš™ï¸ é–‹å§‹è™•ç†ä½¿ç”¨è€… {subscription['user_id']} çš„è¨‚é–± ---")
            success = self._process_subscription_batch(subscription, news_list)
            if success:
                overall_success = True

        print(f"\nâœ… æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„è¨‚é–±ä»»å‹™è™•ç†å®Œç•¢ã€‚")
        return overall_success
    
    def _process_subscription_batch(self, subscription: Dict[str, Any], news_list: List[Dict[str, str]]) -> bool:
        """è™•ç†å–®ä¸€è¨‚é–±ä»»å‹™ - æ‰¹é‡æ”¶é›†å’Œæ¨é€ç‰ˆæœ¬"""
        user_id = subscription['user_id']
        frequency_type = subscription.get('push_frequency_type', 'daily')
        keywords = subscription.get("keywords", [])
        max_articles = db_manager.get_max_articles_for_frequency(frequency_type)
        
        print(f"ğŸ” ç‚ºç”¨æˆ¶ {user_id} æ”¶é›†ç¬¦åˆæ¢ä»¶çš„æ–°è (æœ€å¤š {max_articles} å‰‡)")
        
        # æ”¶é›†ç¬¦åˆæ¢ä»¶çš„æ–°è
        collected_articles = []
        processed_count = 0
        failed_count = 0
        
        for news_item in news_list:
            # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°æœ€å¤§æ•¸é‡
            if len(collected_articles) >= max_articles:
                print(f"ğŸ“Š å·²æ”¶é›†åˆ° {max_articles} å‰‡æ–°èï¼Œåœæ­¢æ”¶é›†")
                break
            
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
            if db_manager.is_article_processed(news_item['link']):
                continue
            
            # æª¢æŸ¥é—œéµå­—åŒ¹é…
            if keywords and not any(keyword.lower() in news_item['title'].lower() for keyword in keywords):
                continue
            
            print(f"ğŸ‘‰ æ‰¾åˆ°ç¬¦åˆé—œéµå­—çš„æ–‡ç« : {news_item['title'][:60]}...")
            processed_count += 1
            
            # å˜—è©¦è™•ç†æ–‡ç« 
            article_data = self._process_single_article(news_item)
            if article_data:
                collected_articles.append(article_data)
                print(f"âœ… æ–‡ç« è™•ç†æˆåŠŸ ({len(collected_articles)}/{max_articles})")
            else:
                failed_count += 1
                print(f"âŒ æ–‡ç« è™•ç†å¤±æ•—")
                
            # å¦‚æœå·²æ”¶é›†è¶³å¤ çš„æ–‡ç« ï¼Œæå‰çµæŸ
            if len(collected_articles) >= max_articles:
                break
        
        # æ¨é€çµæœçµ±è¨ˆ
        print(f"\nğŸ“Š æ”¶é›†çµæœçµ±è¨ˆ:")
        print(f"  - å˜—è©¦è™•ç†: {processed_count} ç¯‡")
        print(f"  - æˆåŠŸæ”¶é›†: {len(collected_articles)} ç¯‡")
        print(f"  - è™•ç†å¤±æ•—: {failed_count} ç¯‡")
        
        if not collected_articles:
            print(f"â„¹ï¸ æœªæ‰¾åˆ°é©åˆç”¨æˆ¶ {user_id} çš„æ–°æ–‡ç« ")
            return False
        
        # æ‰¹é‡æ¨é€åˆ° Discord
        print(f"\nğŸ“¤ é–‹å§‹æ¨é€ {len(collected_articles)} å‰‡æ–°èåˆ° Discord...")
        success, failed_articles = send_batch_to_discord(
            subscription['delivery_target'], 
            collected_articles, 
            subscription
        )
        
        if success:
            # å„²å­˜æˆåŠŸæ¨é€çš„æ–‡ç« ä¸¦è¨˜éŒ„æ­·å²
            successful_articles = [article for article in collected_articles if article not in failed_articles]
            article_ids = []
            
            for article in successful_articles:
                article_id = db_manager.save_new_article(article)
                if article_id:
                    article_ids.append(article_id)
            
            if article_ids:
                # è¨˜éŒ„æ¨é€æ­·å²ï¼ˆæ‰¹é‡è¨˜éŒ„ï¼‰
                batch_success = db_manager.log_push_history(user_id, article_ids)
                if batch_success:
                    print(f"ğŸ“ å·²è¨˜éŒ„æ¨é€æ­·å²: {len(article_ids)} ç¯‡æ–‡ç« ")
                
                # æ¨™è¨˜æ¨é€çª—å£ç‚ºå·²å®Œæˆ
                db_manager.mark_push_window_completed(user_id, frequency_type)
                
                # ç™¼é€æ¨é€ç¸½çµæ¶ˆæ¯
                create_push_summary_message(
                    subscription['delivery_target'],
                    len(successful_articles),
                    len(collected_articles),
                    frequency_type
                )
                
                print(f"ğŸ‰ ç”¨æˆ¶ {user_id} çš„æ¨é€ä»»å‹™å®Œæˆ: {len(successful_articles)} å‰‡æˆåŠŸ")
                return True
        
        print(f"âŒ ç”¨æˆ¶ {user_id} çš„æ¨é€ä»»å‹™å¤±æ•—")
        return False
    
    def _process_single_article(self, news_item: Dict[str, str]) -> Union[Dict[str, Any], None]:
        """è™•ç†å–®ç¯‡æ–‡ç«  - çˆ¬å–å…§å®¹ä¸¦ç”Ÿæˆæ‘˜è¦"""
        try:
            # æŠ“å–å…§å®¹
            content = self.scrape_article_content(news_item['link'])
            if not content:
                return None

            # ç”Ÿæˆæ‘˜è¦
            summary = generate_summary_optimized(content)
            if "[æ‘˜è¦ç”Ÿæˆå¤±æ•—" in summary:
                return None

            # è§£ææ–‡ç« ç™¼å¸ƒæ™‚é–“
            published_at = parse_article_publish_time()
            
            # æ§‹å»ºæ–‡ç« æ•¸æ“š
            article_data = {
                'original_url': news_item['link'], 
                'source': 'yahoo_finance', 
                'title': news_item['title'], 
                'summary': summary,
                'published_at': published_at.isoformat()  # è½‰æ›ç‚º ISO æ ¼å¼å­—ç¬¦ä¸²
            }
            
            return article_data
            
        except Exception as e:
            print(f"âŒ è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

# Create a global scraper manager instance
scraper_manager = NewsScraperManager() 