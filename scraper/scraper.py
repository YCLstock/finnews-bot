import os
import random
import time
from pathlib import Path
from typing import Union, List, Dict, Any
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager

from core.config import settings
from core.database import db_manager
from core.utils import generate_summary_optimized, send_batch_to_discord, create_push_summary_message, parse_article_publish_time

class NewsScraperManager:
    """News scraper manager for FinNews-Bot"""
    
    def __init__(self):
        self.debug_folder = self._create_debug_folder()
    
    def _create_debug_folder(self) -> Path:
        """å‰µå»ºä¸€å€‹ç”¨æ–¼å­˜æ”¾é™¤éŒ¯æˆªåœ–çš„è³‡æ–™å¤¾"""
        path = Path("debug_pages")
        path.mkdir(exist_ok=True)
        return path
    
    def scrape_yahoo_finance_list(self, url: str = None) -> List[Dict[str, str]]:
        """ä½¿ç”¨ requests çˆ¬å– Yahoo Finance çš„æ–°èåˆ—è¡¨é """
        if url is None:
            url = settings.YAHOO_FINANCE_URL
        
        print(f"ğŸ“° æ­£åœ¨å¾ {url} çˆ¬å–æ–°èåˆ—è¡¨...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
        }
        
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.select("li.stream-item")
            result = []
            seen_links = set()
            
            for item in items:
                a_tag = item.select_one('a[href*="/news/"]')
                title_tag = item.select_one('h3')
                if a_tag and title_tag:
                    link = requests.compat.urljoin(url, a_tag['href'])
                    if link not in seen_links:
                        seen_links.add(link)
                        result.append({
                            'title': title_tag.get_text(strip=True), 
                            'link': link
                        })
            
            print(f"ğŸ‘ æˆåŠŸçˆ¬å–åˆ° {len(result)} å‰‡æ–°èæ¨™é¡Œã€‚")
            return result
        except Exception as e:
            print(f"âŒ çˆ¬å–æ–°èåˆ—è¡¨å¤±æ•—: {e}")
            return []
    
    def scrape_article_content(self, url: str) -> Union[str, None]:
        """
        ä½¿ç”¨ Selenium çˆ¬å–å–®ç¯‡æ–°èçš„å…§æ–‡
        - å¼·åŒ–å½è£ä»¥æ‡‰å°åçˆ¬èŸ²æ©Ÿåˆ¶
        - ä½¿ç”¨è¤‡åˆé¸æ“‡å™¨ä»¥æ‡‰å°å¤šç¨®é é¢ç‰ˆé¢
        """
        print(f"ğŸ¦¾ [Selenium] æ­£åœ¨å•Ÿå‹•ç€è¦½å™¨æŠ“å–å®Œæ•´ URL: {url}")
        chrome_options = Options()
        
        # å¼·åŒ–å½è£èˆ‡ç©©å®šæ€§çš„é¸é …
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("window-size=1920x1080")
        chrome_options.add_argument('--ignore-certificate-errors')
        chrome_options.add_argument('--ignore-ssl-errors')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36")

        driver = None
        try:
            # å¼·åˆ¶é‡æ–°ä¸‹è¼‰æœ€æ–°ç‰ˆChromeDriver
            from webdriver_manager.chrome import ChromeDriverManager
            import os
            import shutil
            
            # æ¸…é™¤èˆŠçš„driverç·©å­˜
            cache_path = os.path.expanduser("~/.wdm")
            if os.path.exists(cache_path):
                try:
                    shutil.rmtree(cache_path)
                    print("ğŸ§¹ å·²æ¸…é™¤èˆŠç‰ˆChromeDriverç·©å­˜")
                except:
                    pass
            
            driver_path = ChromeDriverManager().install()
            service = Service(driver_path)
            driver = webdriver.Chrome(service=service, options=chrome_options)
            
            driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
            })
            
            driver.set_page_load_timeout(settings.SCRAPER_TIMEOUT)
            
            print("æ­£åœ¨è¨ªå•é é¢...")
            driver.get(url)

            # è™•ç†åŒæ„è¦–çª—
            try:
                consent_button_locator = (By.CSS_SELECTOR, "button.consent-button[value='agree'], button[name='agree']")
                consent_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable(consent_button_locator)
                )
                print("ğŸª ç™¼ç¾åŒæ„æŒ‰éˆ•ï¼Œæ­£åœ¨é»æ“Š...")
                driver.execute_script("arguments[0].click();", consent_button)
                time.sleep(random.uniform(1, 2))
            except TimeoutException:
                print("ğŸ‘ æœªåœ¨10ç§’å…§ç™¼ç¾æˆ–ä¸éœ€é»æ“ŠåŒæ„æŒ‰éˆ•ï¼Œç¹¼çºŒåŸ·è¡Œã€‚")

            # ç­‰å¾…ä¸¦æŠ“å–ä¸»è¦å…§å®¹
            print("â³ æ­£åœ¨ç­‰å¾…æ–‡ç« ä¸»è¦å…§å®¹å®¹å™¨...")
            content_container_locator = (By.CSS_SELECTOR, '[data-testid="article-content-wrapper"], div.caas-body, div.atoms-wrapper, div.article-wrap.no-bb')
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located(content_container_locator)
            )
            print("âœ… å…§å®¹å®¹å™¨å·²è¼‰å…¥ã€‚")
            
            soup = BeautifulSoup(driver.page_source, "html.parser")
            body = soup.select_one('[data-testid="article-content-wrapper"], div.caas-body, div.atoms-wrapper, div.article-wrap.no-bb')

            if body:
                paragraphs = body.find_all("p")
                if not paragraphs:
                    print(f"âš ï¸ æ‰¾åˆ°å®¹å™¨ {body.get('class')}ï¼Œä½†è£¡é¢æ²’æœ‰ <p> æ¨™ç±¤ã€‚")
                    return None
                
                content_text = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
                
                if content_text:
                    print(f"âœ… æˆåŠŸæ“·å–æ–‡ç« å…§æ–‡ï¼Œç´„ {len(content_text)} å­—ã€‚")
                    return content_text
                else:
                    print("âš ï¸ æ‰¾åˆ° <p> æ¨™ç±¤ï¼Œä½†æ²’æœ‰æœ‰æ•ˆæ–‡å­—å…§å®¹ã€‚")
                    return None
            
            print("âŒ æœªèƒ½æ‰¾åˆ°ä»»ä½•å·²çŸ¥çš„å…§å®¹å®¹å™¨ (data-testid, caas-body, atoms-wrapper, article-wrap)ã€‚")
            return None

        except TimeoutException as e:
            print(f"âŒ æ“·å–å…§æ–‡æ™‚é é¢åŠ è¼‰æˆ–å…ƒç´ ç­‰å¾…è¶…æ™‚ã€‚éŒ¯èª¤è¨Šæ¯: {e.msg}")
            screenshot_path = self.debug_folder / f"TIMEOUT_FAIL_{url.split('/')[-1].replace('.html', '')}.png"
            if driver: 
                driver.save_screenshot(str(screenshot_path))
            print(f"ğŸ“¸ éŒ¯èª¤ç•«é¢å·²æˆªåœ–: {screenshot_path}")
            return None
        except Exception as e:
            print(f"âŒ æ“·å–å…§æ–‡æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            screenshot_path = self.debug_folder / f"GENERAL_FAIL_{url.split('/')[-1].replace('.html', '')}.png"
            if driver: 
                driver.save_screenshot(str(screenshot_path))
            print(f"ğŸ“¸ éŒ¯èª¤ç•«é¢å·²æˆªåœ–: {screenshot_path}")
            return None
        finally:
            if driver:
                driver.quit()
                print("ğŸ§¹ ç€è¦½å™¨å·²é—œé–‰ã€‚")
    
    def process_news_for_subscriptions(self) -> bool:
        """
        ä¸»åŸ·è¡Œå‡½å¼ - è™•ç†æ‰€æœ‰è¨‚é–±çš„æ–°èï¼ˆæ‰¹é‡ç‰ˆæœ¬ï¼‰
        æ ¹æ“šæ–°çš„æ¨é€é »ç‡å’Œæ‰¹é‡è™•ç†éœ€æ±‚å„ªåŒ–
        """
        print("ğŸš€ é–‹å§‹åŸ·è¡Œæ™ºèƒ½æ–°èè™•ç†ä»»å‹™...")
        
        # ç²å–ç¬¦åˆæ¨é€æ¢ä»¶çš„è¨‚é–±
        eligible_subscriptions = db_manager.get_eligible_subscriptions()
        if not eligible_subscriptions:
            print("ğŸŸ¡ ç›®å‰æ²’æœ‰ç¬¦åˆæ¨é€æ™‚é–“çš„è¨‚é–±ä»»å‹™ï¼Œç¨‹å¼çµæŸã€‚")
            return False

        # çˆ¬å–æ–°èåˆ—è¡¨
        news_list = self.scrape_yahoo_finance_list()
        if not news_list:
            print("ğŸŸ¡ æœªèƒ½å¾ Yahoo Finance çˆ¬å–åˆ°ä»»ä½•æ–°èï¼Œç¨‹å¼çµæŸã€‚")
            return False

        # è™•ç†æ¯å€‹ç¬¦åˆæ¢ä»¶çš„è¨‚é–±
        overall_success = False
        for subscription in eligible_subscriptions:
            print(f"\n--- âš™ï¸ é–‹å§‹è™•ç†ä½¿ç”¨è€… {subscription['user_id']} çš„è¨‚é–± ---")
            success = self._process_subscription_batch(subscription, news_list)
            if success:
                overall_success = True

        print(f"\nâœ… æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„è¨‚é–±ä»»å‹™è™•ç†å®Œç•¢ã€‚")
        return overall_success
    
    def _process_subscription_batch(self, subscription: Dict[str, Any], news_list: List[Dict[str, str]]) -> bool:
        """è™•ç†å–®ä¸€è¨‚é–±ä»»å‹™ - æ‰¹é‡æ”¶é›†å’Œæ¨é€ç‰ˆæœ¬"""
        user_id = subscription['user_id']
        frequency_type = subscription.get('push_frequency_type', 'daily')
        keywords = subscription.get("keywords", [])
        max_articles = db_manager.get_max_articles_for_frequency(frequency_type)
        
        print(f"ğŸ” ç‚ºç”¨æˆ¶ {user_id} æ”¶é›†ç¬¦åˆæ¢ä»¶çš„æ–°è (æœ€å¤š {max_articles} å‰‡)")
        
        # æ”¶é›†ç¬¦åˆæ¢ä»¶çš„æ–°è
        collected_articles = []
        processed_count = 0
        failed_count = 0
        
        for news_item in news_list:
            # æª¢æŸ¥æ˜¯å¦å·²é”åˆ°æœ€å¤§æ•¸é‡
            if len(collected_articles) >= max_articles:
                print(f"ğŸ“Š å·²æ”¶é›†åˆ° {max_articles} å‰‡æ–°èï¼Œåœæ­¢æ”¶é›†")
                break
            
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†
            if db_manager.is_article_processed(news_item['link']):
                continue
            
            # æª¢æŸ¥é—œéµå­—åŒ¹é…
            if keywords and not any(keyword.lower() in news_item['title'].lower() for keyword in keywords):
                continue
            
            print(f"ğŸ‘‰ æ‰¾åˆ°ç¬¦åˆé—œéµå­—çš„æ–‡ç« : {news_item['title'][:60]}...")
            processed_count += 1
            
            # å˜—è©¦è™•ç†æ–‡ç« 
            article_data = self._process_single_article(news_item)
            if article_data:
                collected_articles.append(article_data)
                print(f"âœ… æ–‡ç« è™•ç†æˆåŠŸ ({len(collected_articles)}/{max_articles})")
            else:
                failed_count += 1
                print(f"âŒ æ–‡ç« è™•ç†å¤±æ•—")
                
            # å¦‚æœå·²æ”¶é›†è¶³å¤ çš„æ–‡ç« ï¼Œæå‰çµæŸ
            if len(collected_articles) >= max_articles:
                break
        
        # æ¨é€çµæœçµ±è¨ˆ
        print(f"\nğŸ“Š æ”¶é›†çµæœçµ±è¨ˆ:")
        print(f"  - å˜—è©¦è™•ç†: {processed_count} ç¯‡")
        print(f"  - æˆåŠŸæ”¶é›†: {len(collected_articles)} ç¯‡")
        print(f"  - è™•ç†å¤±æ•—: {failed_count} ç¯‡")
        
        if not collected_articles:
            print(f"â„¹ï¸ æœªæ‰¾åˆ°é©åˆç”¨æˆ¶ {user_id} çš„æ–°æ–‡ç« ")
            return False
        
        # æ‰¹é‡æ¨é€åˆ° Discord
        print(f"\nğŸ“¤ é–‹å§‹æ¨é€ {len(collected_articles)} å‰‡æ–°èåˆ° Discord...")
        success, failed_articles = send_batch_to_discord(
            subscription['delivery_target'], 
            collected_articles, 
            subscription
        )
        
        if success:
            # å„²å­˜æˆåŠŸæ¨é€çš„æ–‡ç« ä¸¦è¨˜éŒ„æ­·å²
            successful_articles = [article for article in collected_articles if article not in failed_articles]
            article_ids = []
            
            for article in successful_articles:
                article_id = db_manager.save_new_article(article)
                if article_id:
                    article_ids.append(article_id)
            
            if article_ids:
                # è¨˜éŒ„æ¨é€æ­·å²ï¼ˆæ‰¹é‡è¨˜éŒ„ï¼‰
                batch_success = db_manager.log_push_history(user_id, article_ids)
                if batch_success:
                    print(f"ğŸ“ å·²è¨˜éŒ„æ¨é€æ­·å²: {len(article_ids)} ç¯‡æ–‡ç« ")
                
                # æ¨™è¨˜æ¨é€çª—å£ç‚ºå·²å®Œæˆ
                db_manager.mark_push_window_completed(user_id, frequency_type)
                
                # ç™¼é€æ¨é€ç¸½çµæ¶ˆæ¯
                create_push_summary_message(
                    subscription['delivery_target'],
                    len(successful_articles),
                    len(collected_articles),
                    frequency_type
                )
                
                print(f"ğŸ‰ ç”¨æˆ¶ {user_id} çš„æ¨é€ä»»å‹™å®Œæˆ: {len(successful_articles)} å‰‡æˆåŠŸ")
                return True
        
        print(f"âŒ ç”¨æˆ¶ {user_id} çš„æ¨é€ä»»å‹™å¤±æ•—")
        return False
    
    def _process_single_article(self, news_item: Dict[str, str]) -> Union[Dict[str, Any], None]:
        """è™•ç†å–®ç¯‡æ–‡ç«  - çˆ¬å–å…§å®¹ä¸¦ç”Ÿæˆæ‘˜è¦"""
        try:
            # æŠ“å–å…§å®¹
            content = self.scrape_article_content(news_item['link'])
            if not content:
                return None

            # ç”Ÿæˆæ‘˜è¦
            summary = generate_summary_optimized(content)
            if "[æ‘˜è¦ç”Ÿæˆå¤±æ•—" in summary:
                return None

            # è§£ææ–‡ç« ç™¼å¸ƒæ™‚é–“
            published_at = parse_article_publish_time()
            
            # æ§‹å»ºæ–‡ç« æ•¸æ“š
            article_data = {
                'original_url': news_item['link'], 
                'source': 'yahoo_finance', 
                'title': news_item['title'], 
                'summary': summary,
                'published_at': published_at.isoformat()  # è½‰æ›ç‚º ISO æ ¼å¼å­—ç¬¦ä¸²
            }
            
            return article_data
            
        except Exception as e:
            print(f"âŒ è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

# Create a global scraper manager instance
scraper_manager = NewsScraperManager() 