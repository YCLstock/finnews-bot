# main_bot.py
import os
import random
import time
from pathlib import Path
from typing import Union
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from supabase import create_client, Client
import openai

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager

# --- åˆå§‹åŒ– ---
print("ğŸš€ é–‹å§‹åŸ·è¡Œå®¢è£½åŒ–æ–°è Bot (æœ€çµ‚é»æ“Šç‰ˆ)...")
load_dotenv()

supabase_url = os.environ.get("SUPABASE_URL")
supabase_key = os.environ.get("SUPABASE_SERVICE_KEY")
if not supabase_url or not supabase_key:
    print("âŒ éŒ¯èª¤ï¼šSUPABASE ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®")
    exit()
supabase: Client = create_client(supabase_url, supabase_key)
print("âœ… Supabase client åˆå§‹åŒ–æˆåŠŸï¼")

openai.api_key = os.environ.get("OPENAI_API_KEY")
if openai.api_key:
    print("âœ… OpenAI API Key è®€å–æˆåŠŸï¼")
else:
    print("âš ï¸ è­¦å‘Šï¼šOPENAI_API_KEY æœªè¨­ç½®ï¼Œæ‘˜è¦åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨ã€‚")

# --- æ ¸å¿ƒåŠŸèƒ½ ---
def get_active_subscriptions():
    """å¾ Supabase è®€å–æ‰€æœ‰æ´»èºçš„è¨‚é–±ä»»å‹™"""
    try:
        data = supabase.table("subscriptions").select("*").eq("is_active", True).execute()
        print(f"ğŸ—‚ï¸ å¾è³‡æ–™åº«è®€å–åˆ° {len(data.data)} å€‹æ´»èºçš„è¨‚é–±ä»»å‹™ã€‚")
        return data.data
    except Exception as e:
        print(f"âŒ è®€å–è¨‚é–±ä»»å‹™éŒ¯èª¤: {e}")
        return []

def scrape_yahoo_finance_list(url: str) -> list:
    """ä½¿ç”¨ requests çˆ¬å– Yahoo Finance çš„æ–°èåˆ—è¡¨é """
    print(f"ğŸ“° æ­£åœ¨å¾ {url} çˆ¬å–æ–°èåˆ—è¡¨...")
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'}
    try:
        r = requests.get(url, headers=headers, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, 'html.parser')
        items = soup.select("li.stream-item")
        result = []
        seen_links = set()
        for i in items:
            a_tag = i.select_one('a[href*="/news/"]')
            title_tag = i.select_one('h3')
            if a_tag and title_tag:
                link = requests.compat.urljoin(url, a_tag['href'])
                if link not in seen_links:
                    seen_links.add(link)
                    result.append({'title': title_tag.get_text(strip=True), 'link': link})
        print(f"ğŸ‘ æˆåŠŸçˆ¬å–åˆ° {len(result)} å‰‡æ–°èæ¨™é¡Œã€‚")
        return result
    except Exception as e:
        print(f"âŒ çˆ¬å–æ–°èåˆ—è¡¨å¤±æ•—: {e}")
        return []

def create_debug_folder():
    """å‰µå»ºä¸€å€‹ç”¨æ–¼å­˜æ”¾é™¤éŒ¯æˆªåœ–çš„è³‡æ–™å¤¾"""
    path = Path("debug_pages")
    path.mkdir(exist_ok=True)
    return path

def scrape_article_content(url: str) -> Union[str, None]:
    """
    ã€æœ€çµ‚å„ªåŒ–ç‰ˆã€‘
    ä½¿ç”¨ Selenium çˆ¬å–å–®ç¯‡æ–°èçš„å…§æ–‡ã€‚
    - å¼·åŒ–å½è£ä»¥æ‡‰å°åçˆ¬èŸ²æ©Ÿåˆ¶ã€‚
    - ä½¿ç”¨è¤‡åˆé¸æ“‡å™¨ä»¥æ‡‰å°å¤šç¨®é é¢ç‰ˆé¢ã€‚
    """
    print(f"  - ğŸ¦¾ [Selenium] æ­£åœ¨å•Ÿå‹•ç€è¦½å™¨æŠ“å–å®Œæ•´ URL: {url}")
    chrome_options = Options()
    
    # --- å¼·åŒ–å½è£èˆ‡ç©©å®šæ€§çš„é¸é … ---
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("window-size=1920x1080")
    chrome_options.add_argument('--ignore-certificate-errors')
    chrome_options.add_argument('--ignore-ssl-errors')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36")

    driver = None
    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
            'source': "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        })
        
        driver.set_page_load_timeout(40)
        
        print("    - æ­£åœ¨è¨ªå•é é¢...")
        driver.get(url)

        # --- è™•ç†åŒæ„è¦–çª— ---
        try:
            consent_button_locator = (By.CSS_SELECTOR, "button.consent-button[value='agree'], button[name='agree']")
            consent_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable(consent_button_locator)
            )
            print("    - ğŸª ç™¼ç¾åŒæ„æŒ‰éˆ•ï¼Œæ­£åœ¨é»æ“Š...")
            driver.execute_script("arguments[0].click();", consent_button)
            time.sleep(random.uniform(1, 2))
        except TimeoutException:
            print("    - ğŸ‘ æœªåœ¨10ç§’å…§ç™¼ç¾æˆ–ä¸éœ€é»æ“ŠåŒæ„æŒ‰éˆ•ï¼Œç¹¼çºŒåŸ·è¡Œã€‚")

        # --- ç­‰å¾…ä¸¦æŠ“å–ä¸»è¦å…§å®¹ ---
        print("    - â³ æ­£åœ¨ç­‰å¾…æ–‡ç« ä¸»è¦å…§å®¹å®¹å™¨...")
        # ã€é—œéµä¿®æ”¹ã€‘ä½¿ç”¨è¤‡åˆé¸æ“‡å™¨ï¼Œæ‡‰å°å¤šç¨®å·²çŸ¥çš„é é¢ç‰ˆé¢
        content_container_locator = (By.CSS_SELECTOR, "div.caas-body, div.atoms-wrapper, div.article-wrap.no-bb")
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located(content_container_locator)
        )
        print("    - âœ… å…§å®¹å®¹å™¨å·²è¼‰å…¥ã€‚")
        
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # ã€é—œéµä¿®æ”¹ã€‘å†æ¬¡ä½¿ç”¨è¤‡åˆé¸æ“‡å™¨é€²è¡Œè§£æ
        body = soup.select_one("div.caas-body, div.atoms-wrapper, div.article-wrap.no-bb")

        if body:
            paragraphs = body.find_all("p")
            if not paragraphs:
                print(f"    - âš ï¸ æ‰¾åˆ°å®¹å™¨ {body.get('class')}ï¼Œä½†è£¡é¢æ²’æœ‰ <p> æ¨™ç±¤ã€‚")
                return None
            
            content_text = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
            
            if content_text:
                print(f"    - âœ… æˆåŠŸæ“·å–æ–‡ç« å…§æ–‡ï¼Œç´„ {len(content_text)} å­—ã€‚")
                return content_text
            else:
                print("    - âš ï¸ æ‰¾åˆ° <p> æ¨™ç±¤ï¼Œä½†æ²’æœ‰æœ‰æ•ˆæ–‡å­—å…§å®¹ã€‚")
                return None
        
        print("    - âŒ æœªèƒ½æ‰¾åˆ°ä»»ä½•å·²çŸ¥çš„å…§å®¹å®¹å™¨ (caas-body, body, aoms-wrapper)ã€‚")
        return None

    except TimeoutException as e:
        print(f"    - âŒ æ“·å–å…§æ–‡æ™‚é é¢åŠ è¼‰æˆ–å…ƒç´ ç­‰å¾…è¶…æ™‚ã€‚éŒ¯èª¤è¨Šæ¯: {e.msg}")
        screenshot_path = create_debug_folder() / f"TIMEOUT_FAIL_{url.split('/')[-1].replace('.html', '')}.png"
        if driver: driver.save_screenshot(str(screenshot_path))
        print(f"    - ğŸ“¸ éŒ¯èª¤ç•«é¢å·²æˆªåœ–: {screenshot_path}")
        return None
    except Exception as e:
        print(f"    - âŒ æ“·å–å…§æ–‡æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        screenshot_path = create_debug_folder() / f"GENERAL_FAIL_{url.split('/')[-1].replace('.html', '')}.png"
        if driver: driver.save_screenshot(str(screenshot_path))
        print(f"    - ğŸ“¸ éŒ¯èª¤ç•«é¢å·²æˆªåœ–: {screenshot_path}")
        return None
    finally:
        if driver:
            driver.quit()
            print("    - ğŸ§¹ ç€è¦½å™¨å·²é—œé–‰ã€‚")


def is_article_processed(url: str) -> bool:
    """æª¢æŸ¥æ–‡ç« æ˜¯å¦å·²ç¶“è¢«è™•ç†ä¸¦å„²å­˜é"""
    try:
        result = supabase.table('news_articles').select('id', count='exact').eq('original_url', url).execute()
        return result.count > 0
    except Exception as e:
        print(f"âŒ æª¢æŸ¥æ–‡ç« æ˜¯å¦é‡è¤‡éŒ¯èª¤: {e}")
        return True # ç™¼ç”ŸéŒ¯èª¤æ™‚ï¼Œç•¶ä½œå·²è™•ç†ä»¥é¿å…é‡è¤‡ç™¼é€

import openai

def generate_summary_optimized(content: str) -> str:
    """ä½¿ç”¨ OpenAI API ç”Ÿæˆé‡‘èæ–°èæ‘˜è¦ (å„ªåŒ–ç‰ˆ)"""
    print("    - ğŸ§  æ­£åœ¨ç”Ÿæˆæ‘˜è¦ (ä½¿ç”¨ gpt-3.5-turbo)...")
    if not openai.api_key:
        return "[æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼šAPI Key æœªè¨­å®š]"

    # å„ªåŒ–å¾Œçš„ Promptï¼Œæ›´å…·é«”åœ°æŒ‡å°æ¨¡å‹
    system_prompt = """
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è²¡ç¶“æ–°èç·¨è¼¯ã€‚ä½ çš„ä»»å‹™æ˜¯ç‚ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆä¸€æ®µå°ˆæ¥­ã€å®¢è§€ä¸”ç²¾ç°¡çš„æ‘˜è¦ã€‚

    è«‹éµå¾ªä»¥ä¸‹æŒ‡ç¤ºï¼š
    1.  **é¢¨æ ¼**ï¼šèªæ°£å¿…é ˆä¸­ç«‹ã€å®¢è§€ï¼Œå°ˆæ³¨æ–¼äº‹å¯¦é™³è¿°ï¼Œé¿å…ä»»ä½•çŒœæ¸¬æˆ–æƒ…ç·’æ€§ç”¨èªã€‚
    2.  **æ ¸å¿ƒè¦ç´ **ï¼šæ‘˜è¦å…§å®¹æ‡‰æ¶µè“‹æ–°èçš„å¹¾å€‹é—œéµè¦ç´ ï¼š
        -   **äº‹ä»¶ä¸»è§’**ï¼šæ¶‰åŠçš„ä¸»è¦å…¬å¸ã€äººç‰©æˆ–æ©Ÿæ§‹ã€‚
        -   **æ ¸å¿ƒäº‹ä»¶**ï¼šç™¼ç”Ÿäº†ä»€éº¼é—œéµæ±ºç­–ã€ç™¼å¸ƒæˆ–è®ŠåŒ–ã€‚
        -   **é—œéµæ•¸æ“š**ï¼šæåŠä»»ä½•é‡è¦çš„è²¡å‹™æ•¸æ“šã€ç™¾åˆ†æ¯”æˆ–å¸‚å ´æŒ‡æ¨™ã€‚
        -   **å¸‚å ´å½±éŸ¿**ï¼šç°¡è¿°æ­¤äº‹ä»¶å°ç›¸é—œç”¢æ¥­ã€è‚¡åƒ¹æˆ–å¸‚å ´çš„æ½›åœ¨å½±éŸ¿ã€‚
    3.  **æ ¼å¼è¦æ±‚**ï¼š
        -   ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
        -   æ‘˜è¦é•·åº¦åš´æ ¼æ§åˆ¶åœ¨ 50 åˆ° 150 å­—ä¹‹é–“ã€‚
        -   æ‘˜è¦æ‡‰æ˜¯ä¸€æ®µå®Œæ•´çš„æ®µè½ï¼Œèªæ„é€£è²«ã€‚
    4.  **ç›®æ¨™è®€è€…**ï¼šæ­¤æ‘˜è¦æ˜¯ç‚ºäº†è®“æ²’æœ‰æ™‚é–“é–±è®€å…¨æ–‡çš„æŠ•è³‡äººèƒ½å¿«é€ŸæŒæ¡æ–°èé‡é»ã€‚
    """

    try:
        response = openai.chat.completions.create(
            # 1. æ¨¡å‹æ›´æ›ç‚ºæ›´å…·æˆæœ¬æ•ˆç›Šçš„ gpt-3.5-turbo
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": content}
            ],
            temperature=0.3,  # å°æ–¼æ‘˜è¦ä»»å‹™ï¼Œè¼ƒä½çš„æº«åº¦å¯ä»¥è®“è¼¸å‡ºæ›´ç©©å®šã€æ›´å°ˆæ³¨æ–¼äº‹å¯¦
            max_tokens=600  # ç¨å¾®å¢åŠ  token é™åˆ¶ä»¥ç¢ºä¿æ‘˜è¦èƒ½å®Œæ•´ç”Ÿæˆ
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"    - âŒ æ‘˜è¦å¤±æ•—: {e}")
        return "[æ‘˜è¦ç”Ÿæˆå¤±æ•—]"

def save_new_article(data: dict) -> Union[int, None]:
    """å°‡æ–°è™•ç†çš„æ–‡ç« å„²å­˜åˆ° Supabase"""
    try:
        r = supabase.table("news_articles").insert(data).execute()
        print(f"  - âœ… å„²å­˜æˆåŠŸ: {data['title']}")
        return r.data[0]['id']
    except Exception as e:
        print(f"âŒ å„²å­˜æ–°æ–‡ç« æ™‚éŒ¯èª¤: {e}")
        return None

def send_to_discord(webhook: str, articles: list, sub: dict):
    """å°‡æ ¼å¼åŒ–å¾Œçš„æ–°èæ‘˜è¦ç™¼é€åˆ° Discord Webhook"""
    if not webhook.startswith("https://discord.com/api/webhooks/"):
        print(f"    - âŒ Webhook ä¸æ­£ç¢ºï¼š{webhook}")
        return
    
    fields = [{
        "name": f"**{i+1}. {a['title']}**",
        "value": f"{a['summary']}\n[é»æ­¤é–±è®€åŸæ–‡]({a['original_url']})",
        "inline": False
    } for i, a in enumerate(articles)]
    
    payload = {
        "embeds": [{
            "title": f"æ‚¨è¨‚é–±çš„æ–°è",
            "color": 3447003, # Discord è—è‰²
            "fields": fields,
            "footer": {"text": f"ç™¼é€æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}"}
        }]
    }
    try:
        r = requests.post(webhook, json=payload, timeout=10)
        r.raise_for_status()
        print("    - âœ… æˆåŠŸæ¨é€åˆ° Discord")
    except Exception as e:
        print(f"    - âŒ æ¨é€å¤±æ•—: {e}")

def log_push_history(user_id: str, article_ids: list):
    """è¨˜éŒ„æ¨é€æ­·å²åˆ° Supabase"""
    records = [{"user_id": user_id, "article_id": i} for i in article_ids]
    try:
        supabase.table("push_history").insert(records).execute()
        print(f"    - ğŸ“ å·²ç´€éŒ„æ¨æ’­æ­·å² {len(article_ids)} ç­†")
    except Exception as e:
        print(f"    - âŒ ç´€éŒ„æ¨æ’­æ­·å²å¤±æ•—: {e}")

# --- ä¸»ç¨‹å¼ ---
def main():
    """
    ã€æœ€çµ‚å„ªåŒ–ç‰ˆã€‘
    ä¸»åŸ·è¡Œå‡½å¼ã€‚
    - ä¿®æ­£äº†åŸå…ˆè™•ç†å®Œä¸€å‰‡æ–°èå°±é€€å‡ºçš„é‚è¼¯éŒ¯èª¤ã€‚
    """
    subscriptions = get_active_subscriptions()
    if not subscriptions:
        print("ğŸŸ¡ æ²’æœ‰ä»»ä½•æ´»èºçš„è¨‚é–±ä»»å‹™ï¼Œç¨‹å¼çµæŸã€‚")
        return

    news = scrape_yahoo_finance_list("https://finance.yahoo.com/topic/latest-news/")
    if not news:
        print("ğŸŸ¡ æœªèƒ½å¾ Yahoo Finance çˆ¬å–åˆ°ä»»ä½•æ–°èï¼Œç¨‹å¼çµæŸã€‚")
        return

    for sub in subscriptions:
        print(f"\n--- âš™ï¸ é–‹å§‹è™•ç†ä½¿ç”¨è€… {sub['user_id']} çš„è¨‚é–± ---")
        keywords = sub.get("keywords", [])
        found_news_for_this_user = False

        for item in news:
            if is_article_processed(item['link']):
                continue
            
            if not keywords or any(k.lower() in item['title'].lower() for k in keywords):
                print(f"  - ğŸ‘‰ æ‰¾åˆ°ç¬¦åˆé—œéµå­—çš„æ–‡ç« ï¼Œå˜—è©¦è™•ç†: {item['title']}")
                
                content = scrape_article_content(item['link'])
                if not content:
                    print("  - âŒ å…§å®¹ç²å–å¤±æ•—ï¼Œç¹¼çºŒå°‹æ‰¾ä¸‹ä¸€ç¯‡ã€‚")
                    continue

                summary = generate_summary_optimized(content)
                if "[æ‘˜è¦ç”Ÿæˆå¤±æ•—" in summary:
                    print("  - âŒ æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼Œç¹¼çºŒå°‹æ‰¾ä¸‹ä¸€ç¯‡ã€‚")
                    continue

                article = {'original_url': item['link'], 'source': 'yahoo_finance', 'title': item['title'], 'summary': summary}
                
                new_id = save_new_article(article)
                if new_id:
                    send_to_discord(sub['delivery_target'], [article], sub)
                    log_push_history(sub['user_id'], [new_id])
                    print(f"  - âœ… æˆåŠŸç‚ºä½¿ç”¨è€… {sub['user_id']} æ¨é€æ–°èã€‚")
                    found_news_for_this_user = True
                    # ã€é—œéµä¿®æ”¹ã€‘ä½¿ç”¨ break è·³å‡ºæ–°èè¿´åœˆï¼Œé–‹å§‹è™•ç†ä¸‹ä¸€å€‹ä½¿ç”¨è€…
                    break 
        
        if not found_news_for_this_user:
            print(f"  - â„¹ï¸ æœ¬è¼ªæœªæ‰¾åˆ°é©åˆä½¿ç”¨è€… {sub['user_id']} çš„æ–°æ–‡ç« ã€‚")

    print("\nâœ… æ‰€æœ‰è¨‚é–±ä»»å‹™è™•ç†å®Œç•¢ã€‚")


if __name__ == "__main__":
    main()