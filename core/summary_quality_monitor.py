#!/usr/bin/env python3
"""
æ‘˜è¦å“è³ªç›£æ§æ¨¡çµ„
è² è²¬è¿½è¹¤ã€çµ±è¨ˆå’Œå ±å‘Šæ‘˜è¦ç”Ÿæˆçš„å“è³ªæŒ‡æ¨™
"""

import json
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from pathlib import Path
from dataclasses import dataclass, asdict
import re

logger = logging.getLogger(__name__)

@dataclass
class SummaryQualityMetric:
    """æ‘˜è¦å“è³ªæŒ‡æ¨™"""
    timestamp: str
    title: str
    summary: str
    chinese_ratio: float
    has_english_words: bool
    is_valid: bool
    quality_score: float
    attempt_count: int
    generation_time: float
    success: bool
    error_message: Optional[str] = None

class SummaryQualityMonitor:
    """æ‘˜è¦å“è³ªç›£æ§å™¨"""
    
    def __init__(self, log_file_path: str = "logs/summary_quality.jsonl"):
        """
        åˆå§‹åŒ–å“è³ªç›£æ§å™¨
        
        Args:
            log_file_path: æ—¥èªŒæ–‡ä»¶è·¯å¾‘
        """
        self.log_file_path = Path(log_file_path)
        self.log_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # çµ±è¨ˆæ•¸æ“š
        self.session_stats = {
            'total_attempts': 0,
            'successful_summaries': 0,
            'failed_summaries': 0,
            'chinese_valid': 0,
            'chinese_invalid': 0,
            'retry_needed': 0,
            'total_generation_time': 0.0
        }
        
    def record_summary_attempt(self, metric: SummaryQualityMetric):
        """è¨˜éŒ„æ‘˜è¦ç”Ÿæˆå˜—è©¦"""
        # æ›´æ–°æœƒè©±çµ±è¨ˆ
        self.session_stats['total_attempts'] += 1
        
        if metric.success:
            self.session_stats['successful_summaries'] += 1
        else:
            self.session_stats['failed_summaries'] += 1
            
        if metric.is_valid:
            self.session_stats['chinese_valid'] += 1
        else:
            self.session_stats['chinese_invalid'] += 1
            
        if metric.attempt_count > 1:
            self.session_stats['retry_needed'] += 1
            
        self.session_stats['total_generation_time'] += metric.generation_time
        
        # å¯«å…¥æ—¥èªŒæ–‡ä»¶
        try:
            with open(self.log_file_path, 'a', encoding='utf-8') as f:
                json.dump(asdict(metric), f, ensure_ascii=False)
                f.write('\n')
                
            logger.debug(f"ğŸ“Š å“è³ªæŒ‡æ¨™å·²è¨˜éŒ„: {metric.title[:30]}... (å“è³ªåˆ†æ•¸: {metric.quality_score:.2f})")
            
        except Exception as e:
            logger.error(f"è¨˜éŒ„å“è³ªæŒ‡æ¨™å¤±æ•—: {e}")
    
    def get_session_summary(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰æœƒè©±çš„å“è³ªçµ±è¨ˆæ‘˜è¦"""
        stats = self.session_stats.copy()
        
        # è¨ˆç®—æ¯”ä¾‹
        if stats['total_attempts'] > 0:
            stats['success_rate'] = stats['successful_summaries'] / stats['total_attempts']
            stats['chinese_valid_rate'] = stats['chinese_valid'] / stats['total_attempts']
            stats['retry_rate'] = stats['retry_needed'] / stats['total_attempts']
            stats['avg_generation_time'] = stats['total_generation_time'] / stats['total_attempts']
        else:
            stats['success_rate'] = 0.0
            stats['chinese_valid_rate'] = 0.0
            stats['retry_rate'] = 0.0
            stats['avg_generation_time'] = 0.0
            
        return stats
    
    def analyze_recent_quality(self, hours: int = 24) -> Dict[str, Any]:
        """åˆ†ææœ€è¿‘Nå°æ™‚çš„æ‘˜è¦å“è³ª"""
        if not self.log_file_path.exists():
            return {'error': 'æ²’æœ‰å“è³ªæ•¸æ“š'}
            
        cutoff_time = datetime.now() - timedelta(hours=hours)
        metrics = []
        
        try:
            with open(self.log_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            metric = json.loads(line)
                            metric_time = datetime.fromisoformat(metric['timestamp'])
                            
                            if metric_time >= cutoff_time:
                                metrics.append(metric)
                        except (json.JSONDecodeError, KeyError, ValueError):
                            continue
                            
        except Exception as e:
            logger.error(f"åˆ†æå“è³ªæ•¸æ“šå¤±æ•—: {e}")
            return {'error': f'æ•¸æ“šè®€å–å¤±æ•—: {e}'}
        
        if not metrics:
            return {'error': f'æœ€è¿‘ {hours} å°æ™‚æ²’æœ‰æ•¸æ“š'}
        
        # çµ±è¨ˆåˆ†æ
        total = len(metrics)
        successful = sum(1 for m in metrics if m['success'])
        chinese_valid = sum(1 for m in metrics if m['is_valid'])
        retry_needed = sum(1 for m in metrics if m['attempt_count'] > 1)
        
        quality_scores = [m['quality_score'] for m in metrics if m['success']]
        generation_times = [m['generation_time'] for m in metrics]
        
        analysis = {
            'period_hours': hours,
            'total_attempts': total,
            'successful_summaries': successful,
            'success_rate': successful / total if total > 0 else 0,
            'chinese_valid': chinese_valid,
            'chinese_valid_rate': chinese_valid / total if total > 0 else 0,
            'retry_needed': retry_needed,
            'retry_rate': retry_needed / total if total > 0 else 0,
            'avg_quality_score': sum(quality_scores) / len(quality_scores) if quality_scores else 0,
            'min_quality_score': min(quality_scores) if quality_scores else 0,
            'max_quality_score': max(quality_scores) if quality_scores else 0,
            'avg_generation_time': sum(generation_times) / len(generation_times) if generation_times else 0,
            'sample_recent': metrics[-3:] if len(metrics) >= 3 else metrics  # æœ€è¿‘3å€‹æ¨£æœ¬
        }
        
        return analysis
    
    def generate_quality_report(self) -> str:
        """ç”Ÿæˆå“è³ªå ±å‘Š"""
        session_stats = self.get_session_summary()
        recent_analysis = self.analyze_recent_quality(24)
        
        report_lines = [
            "ğŸ“Š **æ‘˜è¦å“è³ªç›£æ§å ±å‘Š**",
            f"å ±å‘Šæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "ğŸ”„ **ç•¶å‰æœƒè©±çµ±è¨ˆ**:",
            f"  - ç¸½å˜—è©¦æ¬¡æ•¸: {session_stats['total_attempts']}",
            f"  - æˆåŠŸç‡: {session_stats['success_rate']:.1%}",
            f"  - ä¸­æ–‡æœ‰æ•ˆç‡: {session_stats['chinese_valid_rate']:.1%}",
            f"  - é‡è©¦ç‡: {session_stats['retry_rate']:.1%}",
            f"  - å¹³å‡ç”Ÿæˆæ™‚é–“: {session_stats['avg_generation_time']:.2f}ç§’",
            ""
        ]
        
        if 'error' not in recent_analysis:
            report_lines.extend([
                "ğŸ“ˆ **24å°æ™‚è¶¨å‹¢åˆ†æ**:",
                f"  - ç¸½è™•ç†é‡: {recent_analysis['total_attempts']} ç¯‡",
                f"  - æˆåŠŸç‡: {recent_analysis['success_rate']:.1%}",
                f"  - ä¸­æ–‡æœ‰æ•ˆç‡: {recent_analysis['chinese_valid_rate']:.1%}",
                f"  - é‡è©¦ç‡: {recent_analysis['retry_rate']:.1%}",
                f"  - å¹³å‡å“è³ªåˆ†æ•¸: {recent_analysis['avg_quality_score']:.2f}",
                f"  - å“è³ªåˆ†æ•¸ç¯„åœ: {recent_analysis['min_quality_score']:.2f} ~ {recent_analysis['max_quality_score']:.2f}",
                f"  - å¹³å‡ç”Ÿæˆæ™‚é–“: {recent_analysis['avg_generation_time']:.2f}ç§’",
                ""
            ])
            
            # æ·»åŠ æœ€è¿‘æ¨£æœ¬
            if recent_analysis.get('sample_recent'):
                report_lines.append("ğŸ¯ **æœ€è¿‘è™•ç†æ¨£æœ¬**:")
                for i, sample in enumerate(recent_analysis['sample_recent'][-3:], 1):
                    status = "âœ… æˆåŠŸ" if sample['success'] else "âŒ å¤±æ•—"
                    report_lines.append(f"  {i}. {status} | å“è³ª: {sample['quality_score']:.2f} | å˜—è©¦: {sample['attempt_count']}æ¬¡")
                    report_lines.append(f"     æ¨™é¡Œ: {sample['title'][:50]}...")
                    report_lines.append(f"     æ‘˜è¦: {sample['summary'][:50]}...")
                report_lines.append("")
        else:
            report_lines.extend([
                "âŒ **24å°æ™‚åˆ†æ**:",
                f"  - {recent_analysis['error']}",
                ""
            ])
        
        # å“è³ªå»ºè­°ï¼ˆé‡å°æ··åˆèªè¨€ç­–ç•¥ï¼‰
        if session_stats['chinese_valid_rate'] < 0.85:
            report_lines.extend([
                "âš ï¸ **å“è³ªå»ºè­°**:",
                "  - æ‘˜è¦å“è³ªåä½ï¼Œå¯èƒ½åŒ…å«éå¤šç¦ç”¨è‹±æ–‡è©å½™",
                "  - æª¢æŸ¥æ˜¯å¦æœ‰èªæ³•æ€§è‹±æ–‡è©å½™æ»²é€",
                "  - è€ƒæ…®èª¿æ•´å°ˆæ¥­è¡“èªç™½åå–®",
                ""
            ])
        elif session_stats['chinese_valid_rate'] >= 0.92:
            report_lines.extend([
                "ğŸ‰ **å“è³ªå„ªç§€**:",
                "  - æ··åˆèªè¨€æ‘˜è¦å“è³ªç©©å®šï¼Œå°ˆæ¥­æ€§èˆ‡å¯è®€æ€§å¹³è¡¡è‰¯å¥½",
                "  - ä¸­æ–‡ç‚ºä¸»ã€å°ˆæ¥­è¡“èªé©åº¦çš„ç­–ç•¥é‹è¡Œæ­£å¸¸",
                ""
            ])
        
        return "\n".join(report_lines)
    
    def clear_old_logs(self, days: int = 7):
        """æ¸…ç†èˆŠçš„æ—¥èªŒæ•¸æ“š"""
        if not self.log_file_path.exists():
            return
            
        cutoff_time = datetime.now() - timedelta(days=days)
        kept_lines = []
        removed_count = 0
        
        try:
            with open(self.log_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            metric = json.loads(line)
                            metric_time = datetime.fromisoformat(metric['timestamp'])
                            
                            if metric_time >= cutoff_time:
                                kept_lines.append(line)
                            else:
                                removed_count += 1
                        except (json.JSONDecodeError, KeyError, ValueError):
                            # ä¿ç•™ç„¡æ³•è§£æçš„è¡Œ
                            kept_lines.append(line)
                            
            # é‡å¯«æ–‡ä»¶
            with open(self.log_file_path, 'w', encoding='utf-8') as f:
                f.writelines(kept_lines)
                
            logger.info(f"ğŸ“ æ¸…ç†å®Œæˆ: ä¿ç•™ {len(kept_lines)} æ¢è¨˜éŒ„ï¼Œæ¸…ç† {removed_count} æ¢èˆŠè¨˜éŒ„")
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ—¥èªŒå¤±æ•—: {e}")

# å…¨åŸŸå“è³ªç›£æ§å¯¦ä¾‹
_quality_monitor = None

def get_quality_monitor() -> SummaryQualityMonitor:
    """ç²å–å“è³ªç›£æ§å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _quality_monitor
    if _quality_monitor is None:
        _quality_monitor = SummaryQualityMonitor()
    return _quality_monitor

def record_summary_quality(title: str, summary: str, chinese_ratio: float, 
                          has_english_words: bool, is_valid: bool, 
                          quality_score: float, attempt_count: int, 
                          generation_time: float, success: bool, 
                          error_message: Optional[str] = None,
                          detailed_analysis: Optional[dict] = None):
    """ä¾¿åˆ©å‡½æ•¸ï¼šè¨˜éŒ„æ‘˜è¦å“è³ªæŒ‡æ¨™ï¼ˆæ”¯æ´è©³ç´°åˆ†æï¼‰"""
    monitor = get_quality_monitor()
    
    # å¦‚æœæœ‰è©³ç´°åˆ†æï¼Œæ·»åŠ åˆ°éŒ¯èª¤è¨Šæ¯ä¸­
    if detailed_analysis and not success:
        analysis_info = []
        if detailed_analysis.get('forbidden_words'):
            analysis_info.append(f"ç¦ç”¨è©å½™: {detailed_analysis['forbidden_words']}")
        if detailed_analysis.get('unknown_words'):
            analysis_info.append(f"æœªçŸ¥è©å½™: {detailed_analysis['unknown_words']}")
        if analysis_info:
            error_message = f"{error_message or ''} | {'; '.join(analysis_info)}"
    
    metric = SummaryQualityMetric(
        timestamp=datetime.now().isoformat(),
        title=title,
        summary=summary,
        chinese_ratio=chinese_ratio,
        has_english_words=has_english_words,
        is_valid=is_valid,
        quality_score=quality_score,
        attempt_count=attempt_count,
        generation_time=generation_time,
        success=success,
        error_message=error_message
    )
    
    monitor.record_summary_attempt(metric)

# å°ˆæ¥­è¡“èªç™½åå–®ç³»çµ±
ALLOWED_ENGLISH_TERMS = {
    'companies': [
        # ç§‘æŠ€å…¬å¸
        'Apple', 'Microsoft', 'Google', 'Meta', 'Amazon', 'Tesla', 'NVIDIA', 'Intel',
        'Samsung', 'Sony', 'Adobe', 'Oracle', 'IBM', 'Cisco', 'Netflix',
        # é‡‘èæ©Ÿæ§‹
        'JPMorgan', 'Goldman', 'BlackRock', 'Berkshire', 
        # å°ç£å…¬å¸å¸¸ç”¨è‹±æ–‡å
        'TSMC', 'MediaTek', 'ASE', 'Foxconn', 'HTC'
    ],
    'finance_terms': [
        # åŸºæœ¬é‡‘èè¡“èª
        'GDP', 'IPO', 'CEO', 'CFO', 'COO', 'CTO', 'ROI', 'ROE', 'ROA', 'KPI',
        'ESG', 'ETF', 'REIT', 'M&A', 'IPO', 'ICO', 'DeFi', 'NFT',
        # è²¨å¹£å’Œå¸‚å ´
        'USD', 'EUR', 'JPY', 'CNY', 'TWD', 'NYSE', 'NASDAQ', 'S&P',
        # è²¡å ±è¡“èª
        'EBITDA', 'P/E', 'EPS', 'P/B', 'PEG'
    ],
    'tech_terms': [
        # æŠ€è¡“è¡“èª
        'AI', 'ML', 'IoT', '5G', '6G', 'VR', 'AR', 'API', 'SDK', 'SaaS',
        'Cloud', 'Edge', 'Blockchain', 'Web3', 'FinTech', 'RegTech',
        # ç”¢æ¥­è¡“èª
        'EV', 'IC', 'OLED', 'LED', 'CPU', 'GPU', 'RAM', 'SSD', 'HDD'
    ],
    'units': [
        # å–®ä½
        'TB', 'GB', 'MB', 'GHz', 'MHz', 'nm', 'kWh', 'MW', 'GW'
    ]
}

# èªæ³•æ€§è‹±æ–‡è©å½™ï¼ˆæ‡‰è©²é¿å…ï¼‰
FORBIDDEN_ENGLISH_WORDS = [
    'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
    'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
    'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
    'this', 'that', 'these', 'those', 'a', 'an', 'as', 'if', 'when', 'where',
    'what', 'who', 'how', 'why', 'which', 'from', 'up', 'down', 'out', 'off',
    'over', 'under', 'again', 'further', 'then', 'once'
]

def get_all_allowed_terms() -> set:
    """ç²å–æ‰€æœ‰å…è¨±çš„è‹±æ–‡è¡“èª"""
    all_terms = set()
    for category in ALLOWED_ENGLISH_TERMS.values():
        all_terms.update(term.lower() for term in category)
    return all_terms

def validate_mixed_language_summary(text: str) -> tuple[bool, float, bool, dict]:
    """
    é©—è­‰æ··åˆèªè¨€æ‘˜è¦çš„å“è³ª
    
    Returns:
        Tuple[æ˜¯å¦æœ‰æ•ˆ, ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹, åŒ…å«ç¦ç”¨è‹±æ–‡è©å½™, è©³ç´°åˆ†æ]
    """
    if not text or len(text.strip()) < 10:
        return False, 0.0, False, {'error': 'Text too short'}
    
    text = text.strip()
    
    # è¨ˆç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ï¼ˆæ”¹é€²ç‰ˆï¼šåƒ…è¨ˆç®—å­—æ¯+ä¸­æ–‡ï¼Œæ’é™¤æ•¸å­—å’Œç¬¦è™Ÿï¼‰
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
    # åªè¨ˆç®—æ–‡å­—å­—ç¬¦ï¼šä¸­æ–‡å­—ç¬¦ + è‹±æ–‡å­—æ¯
    text_chars_only = re.findall(r'[a-zA-Z\u4e00-\u9fff]', text)
    
    if len(text_chars_only) == 0:
        return False, 0.0, False, {'error': 'No text characters'}
    
    chinese_ratio = len(chinese_chars) / len(text_chars_only)
    
    # æå–æ‰€æœ‰è‹±æ–‡è©å½™
    english_words = re.findall(r'\b[a-zA-Z]+\b', text)
    english_words_lower = [word.lower() for word in english_words]
    
    # åˆ†é¡è‹±æ–‡è©å½™
    allowed_terms = get_all_allowed_terms()
    forbidden_words = []
    allowed_words = []
    unknown_words = []
    
    for word in english_words_lower:
        if word in FORBIDDEN_ENGLISH_WORDS:
            forbidden_words.append(word)
        elif word in allowed_terms:
            allowed_words.append(word)
        else:
            unknown_words.append(word)
    
    # åˆ¤æ–·æ˜¯å¦åŒ…å«ç¦ç”¨è©å½™
    has_forbidden_words = len(forbidden_words) > 0
    
    # è¨ˆç®—è©³ç´°åˆ†æ
    analysis = {
        'chinese_chars': len(chinese_chars),
        'text_chars_only': len(text_chars_only),  # æ›´æ–°ç‚ºæ–°çš„è¨ˆç®—æ–¹å¼
        'english_words_count': len(english_words),
        'forbidden_words': forbidden_words,
        'allowed_words': allowed_words,
        'unknown_words': unknown_words,
        'chinese_ratio': chinese_ratio
    }
    
    # æœ‰æ•ˆæ¢ä»¶ï¼šä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ >= 55% ä¸”ä¸åŒ…å«ç¦ç”¨è‹±æ–‡è©å½™
    # å†æ¬¡èª¿é™æ¨™æº–ï¼šè€ƒæ…®åˆ°å°ç£è²¡ç¶“æ–°èå¸¸ä½¿ç”¨è‹±æ–‡å…¬å¸åå’ŒæŠ€è¡“è¡“èª
    is_valid = chinese_ratio >= 0.55 and not has_forbidden_words
    
    return is_valid, chinese_ratio, has_forbidden_words, analysis

# å‘å¾Œå…¼å®¹çš„å‡½æ•¸ï¼ˆä¿æŒèˆŠçš„æ¥å£ï¼‰
def validate_chinese_text(text: str) -> tuple[bool, float, bool]:
    """
    å‘å¾Œå…¼å®¹çš„é©—è­‰å‡½æ•¸
    
    Returns:
        Tuple[æ˜¯å¦æœ‰æ•ˆ, ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹, åŒ…å«è‹±æ–‡å–®è©]
    """
    is_valid, chinese_ratio, has_forbidden_words, _ = validate_mixed_language_summary(text)
    return is_valid, chinese_ratio, has_forbidden_words